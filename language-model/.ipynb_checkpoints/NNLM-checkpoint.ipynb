{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da45dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47df5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, debug=False):\n",
    "        df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        if debug:\n",
    "            df = df.sample(2000).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(50000).reset_index(drop=True)\n",
    "        # 读取常用停用词\n",
    "        stopwords = [line.strip() for line in open('../../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "        sentences = []\n",
    "        for title in tqdm(df['title']):   \n",
    "            # 去除标点符号\n",
    "            title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "            # jieba分词\n",
    "            sentence_seged = jieba.cut(title.strip())    \n",
    "            outstr = ''\n",
    "            for word in sentence_seged:\n",
    "                if word != '\\t' and word not in stopwords:\n",
    "                    outstr += word\n",
    "                    outstr += ' '\n",
    "            if outstr != '':\n",
    "                sentences.append(outstr)    \n",
    "        # 获取所有词（token）\n",
    "        token_list = list(set(' '.join(sentences).split()))\n",
    "        # token和index互转字典\n",
    "        self.token2idx = {token: i for i, token in enumerate(token_list)}\n",
    "        self.idx2token = {i: token for i, token in enumerate(token_list)}\n",
    "        \n",
    "        # 构造输入和输出，输入是每三个词，输出是这三个词的下一个词，也就是简单的n-gram语言模型（n=3）\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for sen in sentences:\n",
    "            sen = sen.split()\n",
    "            for i in range(len(sen) - 3):\n",
    "                self.inputs.append([self.token2idx[token] for token in sen[i: i + 3]])\n",
    "                self.labels.append([self.token2idx[sen[i + 3]]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回一个x和一个y\n",
    "        return torch.LongTensor(self.inputs[idx]), torch.LongTensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08065633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_step, n_hidden):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.n_step = n_step\n",
    "        # vocab size投影到到embed size的空间中\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 构造一个隐藏层，输入大小为 步长 * embed size，输入大小为n_hidden\n",
    "        self.linear = nn.Linear(n_step * embed_size, n_hidden)\n",
    "        # 将n_hidden投影回vocab size大小\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = X.view(-1, self.n_step * self.embed_size)\n",
    "        X = self.linear(X)\n",
    "        X = torch.tanh(X)\n",
    "        y = self.output(X)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd74330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f14983113844f0a8eb7ab2c9fee0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d1/4_gsqv2176z583_7rmpm27lh0000gn/T/jieba.cache\n",
      "Loading model cost 0.423 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NNLM(\n",
       "  (embedding): Embedding(8169, 256)\n",
       "  (linear): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=8169, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "dataset = MyDataset(debug=True)\n",
    "# 构造dataloader，batch size设置为128\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "model = NNLM(vocab_size=len(dataset.token2idx), embed_size=256, n_step=3, n_hidden=256)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 查看模型\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90246afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss = 8.963268\n",
      "epoch: 2 loss = 7.734292\n",
      "epoch: 3 loss = 6.106933\n",
      "epoch: 4 loss = 4.815814\n",
      "epoch: 5 loss = 3.199423\n",
      "epoch: 6 loss = 1.643874\n",
      "epoch: 7 loss = 0.639873\n",
      "epoch: 8 loss = 0.337727\n",
      "epoch: 9 loss = 0.167328\n",
      "epoch: 10 loss = 0.119759\n",
      "epoch: 11 loss = 0.083111\n",
      "epoch: 12 loss = 0.063293\n",
      "epoch: 13 loss = 0.054640\n",
      "epoch: 14 loss = 0.043630\n",
      "epoch: 15 loss = 0.035315\n",
      "epoch: 16 loss = 0.030352\n",
      "epoch: 17 loss = 0.059278\n",
      "epoch: 18 loss = 0.024511\n",
      "epoch: 19 loss = 0.021302\n",
      "epoch: 20 loss = 0.066659\n"
     ]
    }
   ],
   "source": [
    "# 训练20个epoch\n",
    "for epoch in range(20):\n",
    "    for train_input, train_label in dataloader:\n",
    "        output = model(train_input)\n",
    "        loss = criterion(output, train_label.squeeze_())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch:', epoch + 1, 'loss =', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b550cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "互联网 将成 南非 -> 世界杯\n",
      "曝 官员 墓地 -> 雕龙\n",
      "李湘 透露 差点 -> 放弃\n",
      "节能 空调 国美 -> 降价\n",
      "结束 中考 日 -> 公布\n",
      "俄 官员 称 -> 可能\n",
      "斯台普 斯 各国 -> 歌迷\n",
      "廖碧儿 学 咏春 -> 做\n",
      "假 期间 造好 -> 有望\n",
      "纽约 设计 获 -> 老外\n",
      "广东 高考 日 -> 放榜\n",
      "中 移动 筹备 -> 创业\n",
      "化工行业 产品价格 企稳 -> 关注\n",
      "哈利波 特中美 两国 -> 口碑\n",
      "星梦 逃役 案一审 -> 开庭\n",
      "月 自考 部分 -> 课程\n",
      "沪 指 周线 -> 弱势\n",
      "五号 新 五街 -> 缔造\n",
      "缔造 顶级 家居生活 -> 寐\n",
      "高速 连堵 公里 -> 图\n",
      "山水 青城 月 -> 日\n",
      "周刊 弹性 社交 -> 网络\n",
      "全运 开幕式 揭秘 -> 终极\n",
      "组图 娜塔莉 挺 -> 大肚\n",
      "易碎品 首日 五项 -> 世界纪录\n",
      "素颜 现身 高校 -> 低调\n",
      "诠释 正宗 中国 -> 武侠\n",
      "家人 争执 赌气 -> 跳下\n",
      "旗袍 臀部 畸形 -> 组图\n",
      "家装 公司 陷阱 -> 懂\n",
      "终将 再成 比翼鸟 -> 图\n",
      "名师 指点 托福 -> 口语\n",
      "齐达内 西班牙 媲美 -> 版\n",
      "经历 解释 主持 -> 防务\n",
      "理财 一对 缓行 -> 银行\n",
      "围观 开心果 五一 -> 多重\n",
      "朱莉 匈牙利 购豪宅 -> 入住\n",
      "年期 债息 率 -> 美国\n",
      "宝马 引来 微博 -> 第一\n",
      "现场 盗 充电器 -> 抓\n",
      "客车 劫匪 搏斗 -> 导致\n",
      "袭 胸 称为 -> 寻求\n",
      "舐 受伤 心灵 -> 其母\n",
      "两只 牛牛 幸福 -> 滋味\n",
      "杨威 个人 秀场 -> 诠释\n",
      "获 美国 制片人 -> 工会\n",
      "盼 易建联 展现 -> 统治力\n",
      "换里 贝里 简直 -> 疯\n",
      "上调 西班牙 国际 -> 银行\n",
      "切尔西 官方 宣布 -> 续约\n",
      "卡萨帝 开门 冰箱 -> 降千\n",
      "岁 章子怡 庆生 -> 曝无新\n",
      "精彩 堪比开 年 -> 大戏\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型进行预测，train_input直接是上面代码中的，直接用\n",
    "# 模型输出之后取argmax，再用idx2token转回单词，查看效果，可以看到效果还可以，有上下文关系\n",
    "predict = model(train_input).data.max(1, keepdim=True)[1].squeeze_().tolist()\n",
    "input_list = train_input.tolist()\n",
    "for i in range(len(input_list)):\n",
    "    print(dataset.idx2token[input_list[i][0]] + ' ' +  \n",
    "          dataset.idx2token[input_list[i][1]] + ' ' + \n",
    "          dataset.idx2token[input_list[i][2]] + ' -> ' + dataset.idx2token[predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d73fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
