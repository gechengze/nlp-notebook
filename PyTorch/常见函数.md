### torch.mm()、torch.bmm()、torch.mul()、torch.matmul()、
- torch.mm()：两个矩阵的乘法，必须是两个2D的，可以写成 A @ B
- torch.bmm()：批量矩阵乘法，必须是两个3D的，第一维是批量大小
- torch.mul()：对位相乘，可以广播，可以写成 A * B
- torch.matmul()：多维可以用，可以广播，前面相同的维度会保留

### torch.nn.Xxx、torch.nn.functional.xxx
- 实际功能相同
- nn.functional.xxx是函数接口，而nn.Xxx是nn.functional.xxx的类封装，并且nn.Xxx都继承于nn.Module
- nn.Xxx 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据
- nn.Xxx继承于nn.Module， 能够很好的与nn.Sequential结合使用， 而nn.functional.xxx无法与nn.Sequential结合使用。
- nn.Xxx不需要你自己定义和管理weight；而nn.functional.xxx需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。
- dropout推荐使用nn.Xxx方式，一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用nn.Xxx方式定义dropout，在调用model.eval()之后，model中所有的dropout layer都关闭，但以nn.function.dropout方式定义dropout，在调用model.eval()之后并不能关闭dropout。

### torch.transpose()、torch.permute()、torch.view()、torch.reshape()、torch.contiguous()
- torch.transpose()：交换指定的两个维度的内容
- torch.permute()则可以一次性交换多个维度
- contiguous()：tensor在转置后，底层数组不一定连续，可以用该函数变成连续
- view要求tensor是连续的，不连续会报错，reshape不要求
- 在满足tensor连续性条件时，a.reshape()返回的结果与a.view()相同，否则返回的结果与a.contiguous().view()相同

### torch.repeat()和torch.expand()
- repeat指定了原始张量在各维度上复制的次数，repeat函数会真正的复制数据并存放于内存中。
- expand函数用于将张量中单数维的数据扩展到指定的size，仅能作用于这些单数维的维度上。
- expand函数并不会重新分配内存，返回结果仅仅是原始张量上的一个视图。
