{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82f47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2363d",
   "metadata": {},
   "source": [
    "# 1.准备数据，建立vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "my_vocab = {}\n",
    "\n",
    "# 使用torchtext.vocab.vocab建立中文词表，按字切分，不分词\n",
    "counter = Counter()\n",
    "for string_ in df['zh']:\n",
    "    counter.update(list(string_))\n",
    "my_vocab['zh'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['zh'].set_default_index(my_vocab['zh']['<unk>'])\n",
    "\n",
    "# 使用torchtext.vocab.vocab建立英文词表，用spacy进行分词\n",
    "counter = Counter()\n",
    "for string_ in df['en']:\n",
    "    counter.update(jieba.cut(string_))\n",
    "my_vocab['en'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['en'].set_default_index(my_vocab['en']['<unk>'])\n",
    "\n",
    "# 将中文的字和英文词分别转换成vocab中对应的index\n",
    "def data_process(df):\n",
    "    data = []\n",
    "    for raw_zh, raw_en in zip(df['zh'], df['en']):\n",
    "        zh_tensor_ = torch.LongTensor([my_vocab['zh'][token] for token in list(raw_zh)])\n",
    "        en_tensor_ = torch.LongTensor([my_vocab['en'][token] for token in jieba.cut(raw_en)])\n",
    "        data.append((zh_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(df)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "PAD_IDX = my_vocab['zh']['<pad>']\n",
    "BOS_IDX = my_vocab['zh']['<bos>']\n",
    "EOS_IDX = my_vocab['zh']['<eos>']\n",
    "\n",
    "# collate_fn，传给DataLoader，对于每一个batch，将其中的句子都pad成和最长的一样长，用PAD_IDX填充\n",
    "def generate_batch(data_batch):\n",
    "    zh_batch, en_batch = [], []\n",
    "    for zh_item, en_item in data_batch:\n",
    "        zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return zh_batch, en_batch\n",
    "\n",
    "# 用DataLoader获取train数据迭代器\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2889192",
   "metadata": {},
   "source": [
    "# 2.构建Encoder、Decoder和Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ec31f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(3441, 64)\n",
       "    (rnn): GRU(64, 64)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(6923, 64)\n",
       "    (rnn): GRU(64, 64)\n",
       "    (fc): Linear(in_features=64, out_features=6923, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # encoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps * batch_size * embde_size]，输出[steps * batch_size * hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回encoder的输出，大小为[steps*batch_size*hidden_size]\n",
    "        # 返回encoder GRU隐层的最后一步\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        enc_output, enc_hidden = self.rnn(embedded)\n",
    "        return enc_output, enc_hidden  \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # decoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps*batch_size*embde_size]，输出[steps*batch_size*hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # 全连接层，输出尺寸为decoder vocab size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, hidden):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "        dec_output, hidden = self.rnn(embedded, hidden)\n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output, hidden = self.encoder(src)  # 首先拿到encoder的output和最后一个时间步的隐状态\n",
    "        max_len, batch_size = tgt.shape[0], tgt.shape[1]  \n",
    "        # Seq2Seq output尺寸为[tgt_max_len*batch_size*tgt_vocab_size]\n",
    "        output = torch.zeros(max_len, batch_size, self.decoder.vocab_size).to(device)\n",
    "        # 先拿tgt的第一个时间步，即<bos>开始，输入到decoder中，第一个时刻的hidden为encoder的\n",
    "        # 最后一个时间步的hidden\n",
    "        y = tgt[0, :]  \n",
    "        # 第二步开始，遍历tgt的每一个时间步，decoder输入为上一时刻的预测结果，已经上一时刻的hidden\n",
    "        for t in range(1, max_len):  \n",
    "            y.unsqueeze_(0)\n",
    "            y, hidden = self.decoder(y, hidden)\n",
    "            y.squeeze_(0)\n",
    "            output[t] = y\n",
    "            y = y.max(1)[1]\n",
    "        return output\n",
    "    \n",
    "# 初始化encoder、decoder和Seq2Seq\n",
    "enc = Encoder(vocab_size=len(my_vocab['zh']), embed_size=64, hidden_size=64)\n",
    "dec = Decoder(vocab_size=len(my_vocab['en']), embed_size=64, hidden_size=64)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8632165",
   "metadata": {},
   "source": [
    "# 3.模型参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c198fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,163,211 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# loss为交叉熵，忽略PAD_IDX\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "\n",
    "# 查看模型中有多少可学习的参数\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a09692",
   "metadata": {},
   "source": [
    "# 4.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36e9cee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bc82b1c4cd47ecaa5ba06e07bf9169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 0.012823243544485155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73b185fbe24280b9643589aa8fa1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 , loss: 0.012761838645234119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa9d3acbaa74cd3bcf6371aca44177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in tqdm(train_iter):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch:', epoch + 1, ', loss:', epoch_loss / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ffb79",
   "metadata": {},
   "source": [
    "# 5.使用模型进行翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c619df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 先讲中文输入到encoder中，拿到encoder的hidden，从<bos>依次输入到decoder中，\n",
    "# 直到预测到<eos>停止，或者超过设定的max_len时停止\n",
    "def translate(zh, max_len=10):\n",
    "    zh_idx = [my_vocab['zh']['<bos>']] + my_vocab['zh'].lookup_indices(list(zh)) + [my_vocab['zh']['<eos>']]\n",
    "    zh_idx = torch.tensor(zh_idx, dtype=torch.long, device=device).unsqueeze_(1)\n",
    "    en_bos = my_vocab['en']['<bos>']\n",
    "    enc_output, hidden = model.encoder(zh_idx)\n",
    "    preds = []\n",
    "    y = torch.tensor([en_bos], dtype=torch.long, device=device)\n",
    "    for t in range(max_len):\n",
    "        y.unsqueeze_(1)\n",
    "        y, hidden = model.decoder(y, hidden)\n",
    "        y.squeeze_(1)\n",
    "        y = y.max(1)[1]\n",
    "        if y.item() == my_vocab['en']['<eos>']:\n",
    "            break\n",
    "        preds.append(my_vocab['en'].get_itos()[y.item()])\n",
    "    return ' '.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a2d0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                  \n"
     ]
    }
   ],
   "source": [
    "print(translate('我是一个学生'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd4efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗨。    ==>    Tom                  \n",
      "你好。    ==>    I                  \n",
      "你用跑的。    ==>    I                  \n",
      "等等！    ==>    Tom                  \n",
      "你好。    ==>    I                  \n",
      "让我来。    ==>    I                  \n",
      "我赢了。    ==>    I                  \n",
      "不会吧。    ==>    Tom                  \n",
      "乾杯!    ==>    Tom                  \n",
      "你懂了吗？    ==>    I                  \n",
      "他跑了。    ==>    I                  \n",
      "跳进来。    ==>    I                  \n",
      "我迷失了。    ==>    I                  \n",
      "我退出。    ==>    I                  \n",
      "我沒事。    ==>    I                  \n",
      "听着。    ==>    Tom                  \n",
      "不可能！    ==>    Tom                  \n",
      "没门！    ==>    Tom                  \n",
      "你确定？    ==>    I                  \n",
      "试试吧。    ==>    The                  \n",
      "我们来试试。    ==>    I                  \n",
      "为什么是我？    ==>    I                  \n",
      "去问汤姆。    ==>    I                  \n",
      "冷静点。    ==>    Tom                  \n",
      "公平点。    ==>    Tom                  \n",
      "友善点。    ==>    Tom                  \n",
      "和气点。    ==>    Tom                  \n",
      "联系我。    ==>    I                  \n",
      "联系我们。    ==>    I                  \n",
      "进来。    ==>    Tom                  \n",
      "找到汤姆。    ==>    I                  \n",
      "滾出去！    ==>    Tom                  \n",
      "出去！    ==>    Tom                  \n",
      "走開！    ==>    Tom                  \n",
      "滾！    ==>    The                  \n",
      "走開！    ==>    Tom                  \n",
      "再见！    ==>    Tom                  \n",
      "告辞！    ==>    I                  \n",
      "等一下！    ==>    Tom                  \n",
      "他来了。    ==>    I                  \n",
      "他跑。    ==>    I                  \n",
      "帮我一下。    ==>    I                  \n",
      "帮帮我们吧！    ==>    I                  \n",
      "坚持。    ==>    Tom                  \n",
      "抱抱汤姆！    ==>    I                  \n",
      "我同意。    ==>    I                  \n",
      "我生病了。    ==>    I                  \n",
      "我老了。    ==>    I                  \n",
      "没关系。    ==>    Tom                  \n",
      "是我。    ==>    I                  \n",
      "来加入我们吧。    ==>    I                  \n",
      "留着吧。    ==>    Tom                  \n",
      "吻我。    ==>    I                  \n",
      "完美！    ==>    I                  \n",
      "再见！    ==>    Tom                  \n",
      "閉嘴！    ==>    I                  \n",
      "不管它。    ==>    Tom                  \n",
      "拿走吧。    ==>    Tom                  \n",
      "醒醒！    ==>    Tom                  \n",
      "去清洗一下。    ==>    I                  \n",
      "我们知道。    ==>    I                  \n",
      "欢迎。    ==>    I                  \n",
      "谁赢了？    ==>    Tom                  \n",
      "为什么不？    ==>    Tom                  \n",
      "你跑。    ==>    I                  \n",
      "往后退点。    ==>    Tom                  \n",
      "静静的，别动。    ==>    Tom                  \n",
      "我一无所知。    ==>    I                  \n",
      "把他铐上。    ==>    I                  \n",
      "往前开。    ==>    Tom                  \n",
      "走開！    ==>    Tom                  \n",
      "滾！    ==>    The                  \n",
      "趴下！    ==>    I                  \n",
      "滾！    ==>    The                  \n",
      "醒醒吧。    ==>    Tom                  \n",
      "做得好！    ==>    I                  \n",
      "干的好！    ==>    I                  \n",
      "抓住汤姆。    ==>    I                  \n",
      "抓住他。    ==>    I                  \n",
      "玩得開心。    ==>    I                  \n",
      "他来试试。    ==>    I                  \n",
      "你就随了我的意吧。    ==>    I                  \n",
      "趕快!    ==>    Tom                  \n",
      "快点！    ==>    Tom                  \n",
      "我忘了。    ==>    I                  \n",
      "我放弃。    ==>    I                  \n",
      "我來付錢。    ==>    I                  \n",
      "我很忙。    ==>    I                  \n",
      "我冷。    ==>    I                  \n",
      "我很好。    ==>    I                  \n",
      "我吃飽了。    ==>    I                  \n",
      "我生病了。    ==>    I                  \n",
      "我病了。    ==>    I                  \n",
      "我个子高。    ==>    I                  \n",
      "让我一个人呆会儿。    ==>    I                  \n",
      "走吧。    ==>    The                  \n",
      "我們開始吧！    ==>    I                  \n",
      "我們走吧!    ==>    I                  \n",
      "当心！    ==>    Tom                  \n",
      "她跑。    ==>    Tom                  \n"
     ]
    }
   ],
   "source": [
    "for zh in df['zh'][0: 100]:\n",
    "    print(zh, '   ==>   ', translate(zh, max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
