{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # mac系统问题\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 能用gpu则用gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2363d",
   "metadata": {},
   "source": [
    "# 1.准备数据，建立vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "my_vocab = {}\n",
    "\n",
    "# 使用torchtext.vocab.vocab建立中文词表，按字切分，不分词\n",
    "counter = Counter()\n",
    "for string_ in df['zh']:\n",
    "    counter.update(list(string_))\n",
    "my_vocab['zh'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['zh'].set_default_index(my_vocab['zh']['<unk>'])\n",
    "\n",
    "# 使用torchtext.vocab.vocab建立英文词表，用spacy进行分词\n",
    "counter = Counter()\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "for string_ in df['en']:\n",
    "    counter.update(en_tokenizer(string_))\n",
    "my_vocab['en'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['en'].set_default_index(my_vocab['en']['<unk>'])\n",
    "\n",
    "# 将中文的字和英文词分别转换成vocab中对应的整数\n",
    "def data_process(df):\n",
    "    data = []\n",
    "    for raw_zh, raw_en in zip(df['zh'], df['en']):\n",
    "        zh_tensor_ = torch.tensor([my_vocab['zh'][token] for token in list(raw_zh)],\n",
    "                                  dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([my_vocab['en'][token] for token in en_tokenizer(raw_en)],\n",
    "                                  dtype=torch.long)\n",
    "        data.append((zh_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(df)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "PAD_IDX = my_vocab['zh']['<pad>']\n",
    "BOS_IDX = my_vocab['zh']['<bos>']\n",
    "EOS_IDX = my_vocab['zh']['<eos>']\n",
    "\n",
    "# collate_fn，传给DataLoader，对于每一个batch，将其中的句子都pad成和最长的一样长，用PAD_IDX填充\n",
    "def generate_batch(data_batch):\n",
    "    zh_batch, en_batch = [], []\n",
    "    for zh_item, en_item in data_batch:\n",
    "        zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return zh_batch, en_batch\n",
    "\n",
    "# 用DataLoader获取train数据迭代器\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2889192",
   "metadata": {},
   "source": [
    "# 2.构建Encoder、Decoder和Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec31f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # encoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps*batch_size*embde_size]，输出[steps*batch_size*hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回encoder的输出，大小为[steps*batch_size*hidden_size]\n",
    "        # 返回encoder GRU隐层的最后一步\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        enc_output, enc_hidden = self.rnn(embedded)\n",
    "        return enc_output, enc_hidden  \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # decoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps*batch_size*embde_size]，输出[steps*batch_size*hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # 全连接层，输出尺寸为decoder vocab size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, hidden):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "        dec_output, hidden = self.rnn(embedded, hidden)\n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output, hidden = self.encoder(src)  # 首先拿到encoder的output和最后一个时间步的隐状态\n",
    "        max_len, batch_size = tgt.shape[0], tgt.shape[1]  \n",
    "        # Seq2Seq output尺寸为[tgt_max_len*batch_size*tgt_vocab_size]\n",
    "        output = torch.zeros(max_len, batch_size, self.decoder.vocab_size).to(device)\n",
    "        # 先拿tgt的第一个时间步，即<bos>开始，输入到decoder中，第一个时刻的hidden为encoder的\n",
    "        # 最后一个时间步的hidden\n",
    "        y = tgt[0, :]  \n",
    "        # 第二步开始，遍历tgt的每一个时间步，decoder输入为上一时刻的预测结果，已经上一时刻的hidden\n",
    "        for t in range(1, max_len):  \n",
    "            y.unsqueeze_(0)\n",
    "            y, hidden = self.decoder(y, hidden)\n",
    "            y.squeeze_(0)\n",
    "            output[t] = y\n",
    "            y = y.max(1)[1]\n",
    "        return output\n",
    "    \n",
    "# 初始化encoder、decoder和Seq2Seq\n",
    "enc = Encoder(vocab_size=len(my_vocab['zh']), embed_size=64, hidden_size=64)\n",
    "dec = Decoder(vocab_size=len(my_vocab['en']), embed_size=64, hidden_size=64)\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8632165",
   "metadata": {},
   "source": [
    "# 3.模型参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c198fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,162,824 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型参数\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# loss为交叉熵，忽略PAD_IDX\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "\n",
    "# 查看模型中有多少可学习的参数\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a09692",
   "metadata": {},
   "source": [
    "# 4.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e9cee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 6.883485282760069\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_iter:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch:', epoch + 1, ', loss:', epoch_loss / len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ffb79",
   "metadata": {},
   "source": [
    "# 5.使用模型进行翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c619df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 先讲中文输入到encoder中，拿到encoder的hidden，从<bos>依次输入到decoder中，\n",
    "# 直到预测到<eos>停止，或者超过设定的max_len时停止\n",
    "def translate(zh, max_len=10):\n",
    "    zh_idx = [my_vocab['zh']['<bos>']] + my_vocab['zh'].lookup_indices(list(zh)) + [my_vocab['zh']['<eos>']]\n",
    "    zh_idx = torch.tensor(zh_idx, dtype=torch.long, device=device).unsqueeze_(1)\n",
    "    en_bos = my_vocab['en']['<bos>']\n",
    "    enc_output, hidden = model.encoder(zh_idx)\n",
    "    preds = []\n",
    "    y = torch.tensor([en_bos], dtype=torch.long, device=device)\n",
    "    for t in range(max_len):\n",
    "        y.unsqueeze_(1)\n",
    "        y, hidden = model.decoder(y, hidden)\n",
    "        y.squeeze_(1)\n",
    "        y = y.max(1)[1]\n",
    "        if y.item() == my_vocab['en']['<eos>']:\n",
    "            break\n",
    "        preds.append(my_vocab['en'].get_itos()[y.item()])\n",
    "    return ' '.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2d0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate('我是一个学生'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd4efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗨。    ==>    \n",
      "你好。    ==>    \n",
      "你用跑的。    ==>    \n",
      "等等！    ==>    \n",
      "你好。    ==>    \n",
      "让我来。    ==>    \n",
      "我赢了。    ==>    \n",
      "不会吧。    ==>    \n",
      "乾杯!    ==>    \n",
      "你懂了吗？    ==>    \n",
      "他跑了。    ==>    \n",
      "跳进来。    ==>    \n",
      "我迷失了。    ==>    \n",
      "我退出。    ==>    \n",
      "我沒事。    ==>    \n",
      "听着。    ==>    \n",
      "不可能！    ==>    \n",
      "没门！    ==>    \n",
      "你确定？    ==>    \n",
      "试试吧。    ==>    \n",
      "我们来试试。    ==>    \n",
      "为什么是我？    ==>    \n",
      "去问汤姆。    ==>    \n",
      "冷静点。    ==>    \n",
      "公平点。    ==>    \n",
      "友善点。    ==>    \n",
      "和气点。    ==>    \n",
      "联系我。    ==>    \n",
      "联系我们。    ==>    \n",
      "进来。    ==>    \n",
      "找到汤姆。    ==>    \n",
      "滾出去！    ==>    \n",
      "出去！    ==>    \n",
      "走開！    ==>    \n",
      "滾！    ==>    \n",
      "走開！    ==>    \n",
      "再见！    ==>    \n",
      "告辞！    ==>    \n",
      "等一下！    ==>    \n",
      "他来了。    ==>    \n",
      "他跑。    ==>    \n",
      "帮我一下。    ==>    \n",
      "帮帮我们吧！    ==>    \n",
      "坚持。    ==>    \n",
      "抱抱汤姆！    ==>    \n",
      "我同意。    ==>    \n",
      "我生病了。    ==>    \n",
      "我老了。    ==>    \n",
      "没关系。    ==>    \n",
      "是我。    ==>    \n",
      "来加入我们吧。    ==>    \n",
      "留着吧。    ==>    \n",
      "吻我。    ==>    \n",
      "完美！    ==>    \n",
      "再见！    ==>    \n",
      "閉嘴！    ==>    \n",
      "不管它。    ==>    \n",
      "拿走吧。    ==>    \n",
      "醒醒！    ==>    \n",
      "去清洗一下。    ==>    \n",
      "我们知道。    ==>    \n",
      "欢迎。    ==>    \n",
      "谁赢了？    ==>    \n",
      "为什么不？    ==>    \n",
      "你跑。    ==>    \n",
      "往后退点。    ==>    \n",
      "静静的，别动。    ==>    \n",
      "我一无所知。    ==>    \n",
      "把他铐上。    ==>    \n",
      "往前开。    ==>    \n",
      "走開！    ==>    \n",
      "滾！    ==>    \n",
      "趴下！    ==>    \n",
      "滾！    ==>    \n",
      "醒醒吧。    ==>    \n",
      "做得好！    ==>    \n",
      "干的好！    ==>    \n",
      "抓住汤姆。    ==>    \n",
      "抓住他。    ==>    \n",
      "玩得開心。    ==>    \n",
      "他来试试。    ==>    \n",
      "你就随了我的意吧。    ==>    \n",
      "趕快!    ==>    \n",
      "快点！    ==>    \n",
      "我忘了。    ==>    \n",
      "我放弃。    ==>    \n",
      "我來付錢。    ==>    \n",
      "我很忙。    ==>    \n",
      "我冷。    ==>    \n",
      "我很好。    ==>    \n",
      "我吃飽了。    ==>    \n",
      "我生病了。    ==>    \n",
      "我病了。    ==>    \n",
      "我个子高。    ==>    \n",
      "让我一个人呆会儿。    ==>    \n",
      "走吧。    ==>    \n",
      "我們開始吧！    ==>    \n",
      "我們走吧!    ==>    \n",
      "当心！    ==>    \n",
      "她跑。    ==>    \n"
     ]
    }
   ],
   "source": [
    "for zh in df['zh'][0: 100]:\n",
    "    print(zh, '   ==>   ', translate(zh, max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
