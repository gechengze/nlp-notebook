{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2363d",
   "metadata": {},
   "source": [
    "# 1.准备数据，建立vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2915a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d71e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = set()\n",
    "for en in df['en']:\n",
    "    for i in en.split(' '):\n",
    "        all_tokens.add(i.replace('.', '').replace(',', '').replace('!', '').replace('?', '').replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb2a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class En2Zh(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "        \n",
    "        # 英文按空格切分，创建词表\n",
    "        self.ens = []\n",
    "        counter = Counter()\n",
    "        for en in df['en']:\n",
    "            tokenized = [x.replace('.', '').replace(',', '').replace('!', '').replace('?', '').replace(' ', '') for x in en.split(' ') if x not in ['.', '!', '']]\n",
    "            counter.update(tokenized)\n",
    "            self.ens.append(tokenized)\n",
    "            \n",
    "        self.en_vocab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "        self.en_vocab.set_default_index(self.en_vocab['<unk>'])\n",
    "        \n",
    "        # 中文按字切分，创建词表\n",
    "        counter = Counter()\n",
    "        self.zhs = []\n",
    "        for zh in df['zh']:\n",
    "            zh_list = [x for x in zh if x not in ['.', ',', '。', '！', '？', '!', '?']]\n",
    "            counter.update(zh_list)\n",
    "            self.zhs.append(zh_list)\n",
    "            \n",
    "        self.zh_vocab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "        self.zh_vocab.set_default_index(self.zh_vocab['<unk>'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.zhs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        en_encoded = [self.en_vocab['<bos>']] + [self.en_vocab[token] for token in self.ens[idx]] + [self.en_vocab['<eos>']]\n",
    "        zh_encoded = [self.zh_vocab['<bos>']] + [self.zh_vocab[token] for token in self.zhs[idx]] + [self.zh_vocab['<eos>']]\n",
    "        \n",
    "        return torch.LongTensor(en_encoded), torch.LongTensor(zh_encoded)\n",
    "\n",
    "# collate_fn，传给DataLoader，对于每一个batch，将其中的句子都pad成和最长的一样长，用PAD_IDX填充\n",
    "def collate_fn(data_batch):\n",
    "    en_batch, zh_batch = [], []\n",
    "    for en_item, zh_item in data_batch:\n",
    "        en_batch.append(en_item)\n",
    "        zh_batch.append(zh_item)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=1, batch_first=True)\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=1, batch_first=True)\n",
    "    return en_batch, zh_batch\n",
    "\n",
    "dataset = En2Zh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94f4daef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 4, 3]), tensor([2, 4, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2889192",
   "metadata": {},
   "source": [
    "# 2.构建Encoder、Decoder和Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ec31f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(7214, 512)\n",
       "    (rnn): GRU(512, 512, num_layers=2, batch_first=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(3434, 512)\n",
       "    (rnn): GRU(512, 512, num_layers=2, batch_first=True)\n",
       "    (fc): Linear(in_features=512, out_features=3434, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # encoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps * batch_size * embde_size]，输出[steps * batch_size * hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=2, batch_first=True)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回encoder的输出，大小为[steps*batch_size*hidden_size]\n",
    "        # 返回encoder GRU隐层的最后一步\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        enc_output, enc_hidden = self.rnn(embedded)\n",
    "        return enc_output, enc_hidden  \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size  # decoder vocab size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 将vocab size嵌入到embed size\n",
    "        # GRU循环网络，输入[steps*batch_size*embde_size]，输出[steps*batch_size*hidden_size]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # 全连接层，输出尺寸为decoder vocab size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, hidden):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "        dec_output, dec_hidden = self.rnn(embedded, hidden)\n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output, dec_hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output, hidden = self.encoder(src)  # 首先拿到encoder的output和最后一个时间步的隐状态\n",
    "        batch_size, max_len = tgt.shape[0], tgt.shape[1]  \n",
    "        # Seq2Seq output尺寸为[批量大小 * 步长 * vocab_size]\n",
    "        output = torch.zeros(batch_size, max_len, self.decoder.vocab_size).to(device)\n",
    "        # 先拿tgt的第一个时间步，即<bos>开始，输入到decoder中，\n",
    "        # 第一个时刻的hidden为encoder的最后一个时间步的hidden\n",
    "        y_t = tgt[:, 0]  \n",
    "        # 第二步开始，遍历tgt的每一个时间步，decoder输入为上一时刻的预测结果，以及上一时刻的hidden\n",
    "        for t in range(1, max_len):  \n",
    "            y_t.unsqueeze_(1)\n",
    "            y_t, hidden = self.decoder(y_t, hidden)\n",
    "            y_t.squeeze_(1)\n",
    "            output[:, t, :] = y_t\n",
    "            y_t = y_t.argmax(1)\n",
    "        return output\n",
    "    \n",
    "# 初始化encoder、decoder和Seq2Seq\n",
    "enc = Encoder(vocab_size=len(dataset.en_vocab), embed_size=512, hidden_size=512)\n",
    "dec = Decoder(vocab_size=len(dataset.zh_vocab), embed_size=512, hidden_size=512)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a09692",
   "metadata": {},
   "source": [
    "# 3.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36e9cee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 0.000811\n",
      "epoch: 2 , loss: 0.000746\n",
      "epoch: 3 , loss: 0.000735\n",
      "epoch: 4 , loss: 0.000724\n",
      "epoch: 5 , loss: 0.000715\n",
      "epoch: 6 , loss: 0.000714\n",
      "epoch: 7 , loss: 0.000710\n",
      "epoch: 8 , loss: 0.000703\n",
      "epoch: 9 , loss: 0.000709\n",
      "epoch: 10 , loss: 0.000711\n",
      "epoch: 11 , loss: 0.000696\n",
      "epoch: 12 , loss: 0.000707\n",
      "epoch: 13 , loss: 0.000706\n",
      "epoch: 14 , loss: 0.000697\n",
      "epoch: 15 , loss: 0.000701\n",
      "epoch: 16 , loss: 0.000696\n",
      "epoch: 17 , loss: 0.000712\n",
      "epoch: 18 , loss: 0.000684\n",
      "epoch: 19 , loss: 0.000695\n",
      "epoch: 20 , loss: 0.000693\n",
      "epoch: 21 , loss: 0.000679\n",
      "epoch: 22 , loss: 0.000682\n",
      "epoch: 23 , loss: 0.000667\n",
      "epoch: 24 , loss: 0.000656\n",
      "epoch: 25 , loss: 0.000670\n",
      "epoch: 26 , loss: 0.000661\n",
      "epoch: 27 , loss: 0.000665\n",
      "epoch: 28 , loss: 0.000656\n",
      "epoch: 29 , loss: 0.000650\n",
      "epoch: 30 , loss: 0.000654\n",
      "epoch: 31 , loss: 0.000663\n",
      "epoch: 32 , loss: 0.000663\n",
      "epoch: 33 , loss: 0.000653\n",
      "epoch: 34 , loss: 0.000654\n",
      "epoch: 35 , loss: 0.000648\n",
      "epoch: 36 , loss: 0.000654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1).to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        output = model(src, tgt)\n",
    "        output = output[:, 1:, :].reshape(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch:', epoch + 1, ', loss:', format(epoch_loss / len(dataset), '.6f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ffb79",
   "metadata": {},
   "source": [
    "# 5.翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c619df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 先讲中文输入到encoder中，拿到encoder的hidden，从<bos>依次输入到decoder中，\n",
    "# 直到预测到<eos>停止，或者超过设定的max_len时停止\n",
    "def translate(en, max_len=10):\n",
    "    tokenized = [x.replace('.', '').replace(',', '').replace('!', '').replace('?', '').replace(' ', '') for x in en.split(' ') if x not in ['.', '!', '']]\n",
    "    en_idx = [dataset.en_vocab['<bos>']] + dataset.en_vocab.lookup_indices(tokenized) + [dataset.zh_vocab['<eos>']]\n",
    "    en_idx = torch.tensor(en_idx, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    zh_bos = dataset.zh_vocab['<bos>']\n",
    "    enc_output, hidden = model.encoder(en_idx)\n",
    "    preds = []\n",
    "    y = torch.LongTensor([zh_bos]).to(device)\n",
    "    \n",
    "    for t in range(max_len):\n",
    "        y.unsqueeze_(0)\n",
    "        y, hidden = model.decoder(y, hidden)\n",
    "        y.squeeze_(0)\n",
    "        y = y.argmax(1)\n",
    "        if y.item() == dataset.zh_vocab['<eos>']:\n",
    "            break\n",
    "        preds.append(dataset.zh_vocab.get_itos()[y.item()])\n",
    "    return ''.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a2d0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文： She has a strong wish to work as an interpreter.\n",
      "中文： 她非常想当口译。\n",
      "模型结果： 她非常想译地\n",
      "\n",
      "英文： We watch TV every day.\n",
      "中文： 我们每天看电视。\n",
      "模型结果： 我们隨天看天视\n",
      "\n",
      "英文： There's nobody here.\n",
      "中文： 這裡沒人。\n",
      "模型结果： 沒什麼人\n",
      "\n",
      "英文： Why am I still here?\n",
      "中文： 为什么我还在这里？\n",
      "模型结果： 为什么我还在这里\n",
      "\n",
      "英文： Do you know the difference between a microscope and a telescope?\n",
      "中文： 你知道显微镜和望远镜的差别吗？\n",
      "模型结果： 你知道显微镜和望远镜\n",
      "\n",
      "英文： He played tennis.\n",
      "中文： 他打了网球。\n",
      "模型结果： 网响彎球表\n",
      "\n",
      "英文： They forgot to lock the door.\n",
      "中文： 他们忘了锁门。\n",
      "模型结果： 門被忘了了\n",
      "\n",
      "英文： Will you please turn down the radio?\n",
      "中文： 請你把收音機關小聲一點好嗎？\n",
      "模型结果： 你你博你收會音機嗎\n",
      "\n",
      "英文： How can you say that?\n",
      "中文： 你怎麼能那麼說？\n",
      "模型结果： 你怎麼多\n",
      "\n",
      "英文： How long will you be staying?\n",
      "中文： 你会待多长时间？\n",
      "模型结果： 火子间多久\n",
      "\n",
      "英文： Tom was late for dinner.\n",
      "中文： 湯姆晚餐遲到了。\n",
      "模型结果： 湯姆早上遲回來\n",
      "\n",
      "英文： He occasionally visited me.\n",
      "中文： 他偶尔会来拜访我。\n",
      "模型结果： 舞拜着我\n",
      "\n",
      "英文： I want to be the one who decides.\n",
      "中文： 我想成为决策的人。\n",
      "模型结果： 我想成成为策的\n",
      "\n",
      "英文： Why are you still unmarried?\n",
      "中文： 你為甚麼還不結婚？\n",
      "模型结果： 为甚麼還不結束\n",
      "\n",
      "英文： She glanced shyly at the young man.\n",
      "中文： 她羞怯地看了一眼那個年輕人。\n",
      "模型结果： 看當開了五些年輕女人\n",
      "\n",
      "英文： It's time to talk.\n",
      "中文： 到談話的時間了。\n",
      "模型结果： 時間過冰候\n",
      "\n",
      "英文： Even a child could do it.\n",
      "中文： 连小孩儿都会做。\n",
      "模型结果： 有什天不做下\n",
      "\n",
      "英文： Eating lunch with you would make me happy.\n",
      "中文： 跟你一起吃午飯會讓我高興。\n",
      "模型结果： 跟你這午時會讓你高興\n",
      "\n",
      "英文： What is that?\n",
      "中文： 那是什么？\n",
      "模型结果： 甚麼\n",
      "\n",
      "英文： I don't know your reasons for not liking Tom.\n",
      "中文： 我不知道你不喜欢汤姆的理由。\n",
      "模型结果： 我不知道你不喜欢汤姆\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "\n",
    "for i in df.sample(20).index:   \n",
    "    print('英文：', df['en'][i])\n",
    "    print('中文：', df['zh'][i])\n",
    "    print('模型结果：', translate(df['en'][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4a349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
