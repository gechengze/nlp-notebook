{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8063306",
   "metadata": {},
   "source": [
    "#### RoBERTa：Robustly optimized BERT approach\n",
    "- 相对于BERT：\n",
    "- 用了更多的训练语料；\n",
    "- 训练上：\n",
    "    - 采用动态【MASK】；\n",
    "    - 去掉下一句预测的NSP任务；\n",
    "    - 更大的batch_size；\n",
    "    - 文本编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe629a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1964a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pretrained_file = '../../models/roberta-base'\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_file)\n",
    "model = RobertaModel.from_pretrained(pretrained_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb37050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ceb1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('how are you', return_tensors='pt')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a80f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1146,  0.1103, -0.0149,  ..., -0.0809, -0.0018, -0.0271],\n",
       "         [-0.0225,  0.1612,  0.0556,  ...,  0.5366,  0.1196,  0.1576],\n",
       "         [ 0.0532, -0.0020,  0.0370,  ..., -0.4887,  0.1641,  0.2736],\n",
       "         ...,\n",
       "         [-0.1586,  0.0837,  0.1302,  ...,  0.3970,  0.1715, -0.0848],\n",
       "         [-0.1065,  0.1044, -0.0383,  ..., -0.1068, -0.0015, -0.0517],\n",
       "         [ 0.0059,  0.0758,  0.1228,  ...,  0.1037,  0.0075,  0.0976]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-2.8348e-03, -1.8850e-01, -2.1461e-01, -1.1530e-01,  1.3189e-01,\n",
       "          2.3539e-01,  2.7159e-01, -6.0856e-02, -8.3708e-02, -1.9298e-01,\n",
       "          2.6565e-01, -4.3355e-05, -1.1200e-01,  1.4636e-01, -1.4502e-01,\n",
       "          4.9390e-01,  2.0317e-01, -5.3051e-01,  7.5674e-02, -3.7784e-02,\n",
       "         -2.8706e-01,  7.9115e-02,  4.9498e-01,  3.6626e-01,  1.0755e-01,\n",
       "          4.5707e-02, -1.5804e-01,  1.3338e-02,  1.5470e-01,  2.6411e-01,\n",
       "          3.0425e-01,  3.5527e-02,  1.1397e-01,  2.5932e-01, -2.5477e-01,\n",
       "          6.6820e-02, -3.4011e-01,  8.4366e-03,  2.8969e-01, -1.9251e-01,\n",
       "         -8.1795e-02,  1.7997e-01,  1.9420e-01, -1.8720e-01, -1.0835e-01,\n",
       "          4.2660e-01,  2.5290e-01,  4.4049e-02, -1.5877e-01, -9.2707e-02,\n",
       "         -3.3307e-01,  3.7266e-01,  3.0196e-01,  2.2397e-01, -6.3288e-02,\n",
       "          4.3699e-02, -9.3048e-02,  2.8365e-01, -7.5851e-02, -7.6650e-02,\n",
       "         -1.2443e-01, -2.3357e-01,  7.3141e-03, -6.6388e-02,  4.7960e-02,\n",
       "         -1.3959e-01,  1.0733e-01, -1.6085e-01, -1.3563e-01,  3.1196e-02,\n",
       "         -1.1300e-01,  1.7004e-01,  1.6964e-01, -3.3313e-01, -3.1102e-01,\n",
       "          7.0277e-02, -6.3457e-01, -9.6144e-02,  3.4468e-01,  4.6423e-01,\n",
       "         -8.3255e-02,  2.4088e-01,  3.9045e-02,  2.1656e-01, -1.2806e-02,\n",
       "         -7.3343e-02, -4.6851e-02, -1.4373e-01,  1.4100e-01,  2.6473e-01,\n",
       "         -2.1426e-01, -4.6240e-01,  4.3216e-02,  3.9133e-02, -9.2348e-02,\n",
       "          2.2254e-02, -1.1372e-02, -1.1127e-01, -1.8471e-01, -1.8038e-01,\n",
       "          9.9089e-02, -2.4278e-01, -1.5152e-01,  2.6573e-01,  4.7390e-04,\n",
       "         -1.6355e-01,  8.2375e-04,  3.2038e-01,  5.0321e-02, -1.0096e-01,\n",
       "         -2.1142e-01,  4.7146e-01,  3.5390e-01, -3.4947e-02, -1.2649e-03,\n",
       "          1.7674e-01,  1.5086e-01, -2.9137e-01,  4.5807e-01, -3.3300e-01,\n",
       "         -3.9866e-03, -1.0464e-01,  1.4464e-01,  1.9178e-01, -2.0823e-01,\n",
       "          3.0469e-01,  1.5175e-01,  3.0945e-01,  2.1146e-01,  1.4162e-01,\n",
       "         -4.4261e-02,  1.3641e-01, -1.3945e-01,  1.5955e-01,  2.3872e-01,\n",
       "          1.2040e-01, -4.9609e-03, -3.4112e-01, -2.6020e-01,  3.0965e-01,\n",
       "          3.4374e-01,  1.3289e-01, -6.3243e-02,  2.0233e-01,  1.0470e-01,\n",
       "          2.4754e-01,  1.4736e-01, -4.4511e-01,  4.2347e-02,  3.5871e-01,\n",
       "          1.2213e-01,  1.5714e-01, -1.1579e-01, -3.0872e-01, -2.8117e-01,\n",
       "         -1.1094e-01,  5.6936e-02, -3.6070e-01, -1.2276e-01,  3.9022e-01,\n",
       "          8.4130e-02, -7.2016e-02, -1.9839e-01, -2.2041e-01, -1.1239e-02,\n",
       "         -1.2389e-01,  6.9024e-03,  9.8317e-02, -7.9866e-02, -4.6790e-01,\n",
       "         -1.3635e-01, -5.3511e-01, -9.6947e-02,  2.2063e-01, -3.5547e-01,\n",
       "          3.0791e-01, -2.9423e-01,  1.0540e-01,  4.1336e-01,  5.9913e-02,\n",
       "         -1.8542e-02, -2.4088e-01, -1.6369e-02,  9.3561e-02,  3.2514e-01,\n",
       "          2.9757e-01, -4.3569e-01,  1.0774e-01,  1.4302e-01,  3.1605e-01,\n",
       "          1.1054e-01, -8.9299e-02, -1.1941e-01,  1.7482e-01, -2.1888e-01,\n",
       "          1.7849e-01, -2.2932e-01,  2.0647e-01, -2.5988e-01, -2.5896e-01,\n",
       "          3.1579e-01, -4.5348e-01, -7.6572e-02,  8.1546e-02,  2.5454e-01,\n",
       "          6.5838e-03, -5.5890e-02, -1.0195e-01,  1.8580e-01,  1.7586e-01,\n",
       "          1.2009e-01, -4.2580e-01,  3.1833e-01, -3.5225e-02, -3.6717e-02,\n",
       "         -3.6468e-02,  1.8630e-01,  2.5572e-01,  9.9702e-02, -3.8584e-01,\n",
       "         -1.5646e-01,  1.3589e-01,  3.1458e-01, -2.6697e-01,  1.6693e-01,\n",
       "         -3.3316e-01, -4.3700e-01, -1.1081e-01,  2.4861e-01,  2.2493e-01,\n",
       "          1.9945e-01, -3.0226e-01,  1.7410e-01, -1.5682e-01, -4.5891e-01,\n",
       "         -3.8876e-01, -1.3655e-01,  2.3969e-01,  1.7983e-01,  1.6895e-01,\n",
       "          2.5715e-01,  3.0751e-02,  1.1278e-01,  1.6356e-01,  1.6140e-01,\n",
       "         -1.3235e-01,  1.5606e-01, -3.6884e-01, -4.3860e-02, -3.2912e-01,\n",
       "         -2.1013e-01, -2.2653e-01,  4.2579e-01, -2.5797e-01,  2.6423e-01,\n",
       "          4.3591e-01, -3.3986e-01, -1.0784e-01,  1.4810e-01,  1.8771e-01,\n",
       "          5.8792e-02, -9.3627e-02,  2.0970e-01,  2.0622e-01, -1.0515e-01,\n",
       "          2.8239e-01, -4.3975e-02,  2.9693e-01,  1.5088e-01,  4.6862e-02,\n",
       "          1.6996e-01,  1.3566e-01, -1.3045e-01,  8.1460e-02,  1.2664e-02,\n",
       "         -3.3464e-02, -2.8986e-01, -1.4230e-01,  2.3894e-01, -7.3702e-02,\n",
       "          2.3693e-02, -1.6792e-01, -2.9238e-02,  1.1320e-02,  4.2676e-01,\n",
       "         -3.7155e-01,  2.7734e-01,  5.9552e-02,  1.4843e-01, -2.3717e-01,\n",
       "         -2.4174e-01,  1.2127e-01,  2.3405e-01, -4.7163e-01,  4.4663e-02,\n",
       "          1.1726e-01,  9.0709e-02,  2.1785e-01,  2.8467e-01,  4.3686e-04,\n",
       "         -1.1491e-01,  5.6653e-01, -1.5376e-01, -1.3448e-01,  2.6769e-01,\n",
       "         -3.0039e-01, -3.0043e-01,  2.7399e-01, -1.9757e-02,  3.1912e-01,\n",
       "          1.4622e-01,  5.6607e-02,  6.1616e-02, -6.1817e-01,  8.0240e-02,\n",
       "         -4.8122e-01, -1.5592e-02,  5.3547e-02, -8.4474e-02, -2.3283e-01,\n",
       "          1.6090e-01,  3.1869e-01, -2.3078e-01, -1.9177e-02,  2.1890e-01,\n",
       "          4.8886e-02, -1.2004e-01,  4.8328e-01, -1.8756e-03,  2.2632e-01,\n",
       "         -5.1116e-02,  2.3901e-01, -2.3146e-01,  2.6983e-01, -2.9126e-01,\n",
       "         -1.2003e-01,  6.2455e-02,  7.8869e-02,  4.1980e-02, -7.5116e-02,\n",
       "         -3.9176e-01,  2.5679e-01, -1.1382e-02, -7.3775e-02, -5.6824e-02,\n",
       "          1.0898e-01, -2.5817e-02,  7.3328e-02,  8.8274e-02,  3.6668e-01,\n",
       "          2.4559e-01, -1.2493e-02, -4.1410e-01, -8.9574e-02, -1.2473e-01,\n",
       "          7.0269e-02,  6.0476e-02,  1.4108e-02,  4.8733e-01, -7.1245e-02,\n",
       "          2.6076e-02, -1.5084e-01,  2.8914e-01,  2.2343e-01,  1.7564e-01,\n",
       "          1.6365e-01,  5.4786e-02,  1.8040e-01, -4.7131e-02, -1.8931e-02,\n",
       "         -1.5203e-01, -2.5458e-01, -2.9443e-01,  2.3336e-01, -2.5514e-01,\n",
       "         -1.6871e-01,  1.6014e-01,  2.0769e-01, -1.6111e-01,  1.8165e-01,\n",
       "          3.4196e-01,  1.5221e-01, -1.6557e-01,  3.1800e-01, -7.6828e-02,\n",
       "          6.5035e-02,  3.0127e-01, -8.3677e-02,  2.0537e-01,  5.4490e-01,\n",
       "          2.5054e-01, -4.2671e-01, -5.8351e-02, -2.2732e-01,  1.4475e-02,\n",
       "          2.7506e-01, -1.2389e-01,  2.1066e-01,  3.8047e-01,  3.0992e-01,\n",
       "          4.9357e-01, -1.5482e-02, -1.1453e-01,  1.1932e-01,  2.3738e-01,\n",
       "          2.3809e-02, -2.0668e-01, -1.7490e-01,  3.0054e-01,  9.5040e-02,\n",
       "         -1.6448e-01,  8.2032e-03, -1.6251e-01,  4.5058e-02, -1.6121e-01,\n",
       "         -4.2871e-01,  4.4034e-02,  1.6746e-01, -5.0884e-01,  1.2617e-01,\n",
       "         -3.1699e-01,  3.6625e-02, -2.4639e-01,  2.6873e-01, -2.5727e-01,\n",
       "         -1.4897e-01,  4.3529e-01, -5.5864e-02,  2.7185e-02, -1.9407e-01,\n",
       "         -1.6114e-01,  2.9191e-02, -1.6527e-03, -7.2602e-02, -4.9691e-03,\n",
       "          3.5502e-01, -1.4824e-01,  7.0599e-02,  2.8513e-02,  2.1469e-01,\n",
       "         -7.3574e-02,  1.7018e-01, -2.6030e-03, -1.4642e-01, -4.1492e-01,\n",
       "          1.6521e-01, -2.2469e-01, -4.2976e-01, -4.2113e-01,  4.1487e-01,\n",
       "         -1.7290e-01, -2.8811e-01, -2.2153e-01, -2.5111e-01,  5.3529e-02,\n",
       "          2.4050e-01,  4.7290e-01, -4.0037e-01, -5.9299e-02,  4.9183e-01,\n",
       "         -5.9624e-02, -2.3099e-01,  2.7614e-01,  2.0807e-01, -3.2530e-01,\n",
       "          3.7258e-01,  2.6673e-01, -6.3106e-02,  5.1716e-02,  5.4289e-01,\n",
       "          1.1893e-01,  2.1599e-01, -2.1227e-01,  4.7760e-01, -2.6859e-01,\n",
       "          3.2262e-01, -1.7062e-01, -1.9771e-01, -2.4215e-01, -3.1962e-02,\n",
       "          3.6125e-01,  1.9359e-01, -4.5894e-01, -1.3200e-01,  4.3003e-02,\n",
       "          3.3126e-01, -4.2145e-01, -6.0110e-02,  4.1000e-02, -3.6422e-01,\n",
       "          1.2588e-01,  1.3854e-01,  2.4897e-01, -4.2243e-01, -6.7698e-02,\n",
       "          4.1120e-01, -3.4483e-01,  1.3896e-01,  3.0409e-01,  8.2978e-02,\n",
       "          3.6610e-01, -6.5310e-02, -8.2796e-03,  5.0149e-02, -2.6011e-01,\n",
       "         -2.2435e-02,  1.2717e-01,  5.8494e-01,  1.4487e-01, -4.0354e-01,\n",
       "          1.1067e-01,  2.6653e-01, -1.8659e-01,  3.3972e-01, -1.0567e-01,\n",
       "         -3.7917e-02,  2.6797e-01, -5.2882e-02,  1.6860e-01, -9.7774e-02,\n",
       "         -2.3715e-01, -3.5562e-01,  4.1702e-01, -1.9961e-01, -1.1364e-01,\n",
       "         -2.0353e-01, -1.0914e-01, -1.9304e-01,  9.5546e-03, -3.9396e-01,\n",
       "          3.6519e-01,  1.3993e-01, -2.1809e-01, -9.8051e-02, -1.0919e-01,\n",
       "         -2.0547e-01, -2.0266e-01, -2.9378e-01,  4.2927e-01, -1.8953e-01,\n",
       "         -4.9567e-01,  2.4299e-01, -1.3043e-02,  3.6800e-01,  4.0744e-03,\n",
       "          1.0481e-01, -6.7964e-02,  1.2875e-01,  8.9863e-02, -8.7973e-02,\n",
       "          2.8249e-01,  9.1169e-02, -5.9373e-01, -1.3107e-01, -2.6811e-01,\n",
       "          9.8535e-02,  2.3464e-01, -3.9614e-01,  1.5656e-02,  3.6724e-02,\n",
       "          2.0153e-01,  1.1375e-02, -1.2173e-01, -7.0875e-02,  4.3149e-01,\n",
       "          2.6859e-01,  3.0436e-01,  1.3308e-01,  2.3106e-01, -4.5165e-03,\n",
       "         -3.6247e-01,  1.8061e-02,  9.4915e-02, -1.9820e-01,  4.6976e-01,\n",
       "         -1.2277e-01, -3.9047e-01, -7.1323e-02,  4.2497e-01,  9.7055e-02,\n",
       "         -2.6554e-02, -3.3067e-02,  2.3281e-01,  1.9053e-01, -1.2797e-01,\n",
       "          2.1960e-01, -8.9729e-03, -1.4726e-01, -1.5024e-01,  9.6500e-02,\n",
       "         -2.3059e-01,  3.7448e-02, -1.4284e-01, -2.9569e-03, -2.2259e-01,\n",
       "          9.0303e-03, -2.2012e-01,  3.0411e-01, -3.5056e-01,  1.1357e-01,\n",
       "          3.1747e-02,  3.2109e-01, -3.7659e-01, -1.7942e-01, -2.8157e-02,\n",
       "          2.0363e-01,  2.7066e-01,  3.8169e-01,  2.1824e-02,  5.1432e-02,\n",
       "         -2.0284e-01, -2.8634e-01,  5.2265e-02, -2.1432e-01,  1.1748e-01,\n",
       "          8.6978e-02,  2.7207e-01, -3.1238e-01, -2.0362e-01,  2.3377e-01,\n",
       "         -5.4622e-02, -1.4171e-01,  4.9318e-01,  2.5240e-01,  1.8076e-01,\n",
       "          2.9939e-02,  2.3065e-01,  2.8543e-02, -1.6701e-01, -1.1991e-01,\n",
       "         -3.0575e-01,  1.0910e-01, -1.1004e-01, -4.9718e-02, -6.8488e-02,\n",
       "         -1.1613e-01, -2.6441e-01, -1.9368e-01,  1.5138e-01,  1.4259e-01,\n",
       "          3.0952e-02, -5.1871e-02, -1.8964e-02, -3.0977e-01,  3.2418e-01,\n",
       "          1.8257e-02,  1.1345e-01, -3.2933e-02,  5.4759e-02, -1.6449e-01,\n",
       "          2.4663e-01,  2.1938e-01,  5.0557e-02, -2.0592e-01, -4.8105e-02,\n",
       "         -3.3525e-01, -3.7126e-01,  2.9869e-02,  1.6168e-01,  1.3227e-01,\n",
       "         -9.2244e-02, -2.8055e-01,  5.7758e-03, -1.2485e-01,  1.9057e-01,\n",
       "          1.8671e-02, -1.8965e-01, -8.9001e-02, -7.3102e-02, -2.7037e-02,\n",
       "          9.7274e-02, -2.2742e-01, -2.0173e-01, -1.3191e-01, -8.9041e-02,\n",
       "         -8.0196e-02,  3.6282e-01, -8.2313e-02,  3.2301e-01, -1.6621e-01,\n",
       "          2.9981e-02, -2.1863e-01,  1.5416e-01, -7.0528e-02,  7.8769e-02,\n",
       "          3.0648e-01, -4.6061e-01, -1.2646e-01, -2.7547e-02, -1.9933e-01,\n",
       "         -1.6299e-01, -1.1201e-01, -1.8919e-02,  2.2978e-01, -3.7508e-01,\n",
       "          2.0549e-01, -1.2558e-01,  2.0741e-01, -7.4029e-02, -2.5238e-01,\n",
       "         -1.6511e-01, -1.0579e-03,  2.9722e-01, -3.3988e-01, -2.3128e-01,\n",
       "         -3.1352e-01, -1.2131e-01, -1.1570e-01, -2.9116e-01,  4.4779e-01,\n",
       "         -1.1711e-01, -5.9108e-02,  3.8968e-02,  4.3492e-01,  2.1890e-01,\n",
       "          1.7709e-01,  2.5733e-01,  1.5447e-02,  1.2343e-02,  1.1580e-01,\n",
       "         -4.9933e-01,  2.8145e-01, -2.6542e-01, -1.7272e-01,  2.0918e-04,\n",
       "          1.4980e-01, -5.8473e-02,  3.9033e-02, -1.5795e-01, -1.1659e-01,\n",
       "          2.3245e-01, -3.9180e-01, -1.6454e-02,  3.0201e-01,  1.5689e-01,\n",
       "         -2.9235e-01,  3.0020e-02,  1.2322e-01,  3.8744e-01,  5.0056e-02,\n",
       "         -2.4898e-01,  1.4628e-01, -3.6670e-01, -1.9514e-02, -2.1036e-01,\n",
       "         -3.3647e-01,  1.9012e-01, -1.1920e-01,  3.9577e-02, -6.7328e-02,\n",
       "         -3.0076e-01,  2.0926e-01, -6.3638e-02, -3.9904e-02,  4.5228e-01,\n",
       "          7.4620e-02, -1.1017e-01,  1.5910e-01,  3.6012e-02,  1.9745e-02,\n",
       "         -9.8928e-02,  2.8636e-01,  2.0934e-01, -3.2661e-01,  7.1475e-02,\n",
       "         -1.7098e-01, -2.6441e-02, -1.4120e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e7cc2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08119069039821625,\n",
       "  'token': 47427,\n",
       "  'token_str': ' sys',\n",
       "  'sequence': 'from sys import datetime'},\n",
       " {'score': 0.06532621383666992,\n",
       "  'token': 86,\n",
       "  'token_str': ' time',\n",
       "  'sequence': 'from time import datetime'},\n",
       " {'score': 0.051917605102062225,\n",
       "  'token': 750,\n",
       "  'token_str': ' history',\n",
       "  'sequence': 'from history import datetime'},\n",
       " {'score': 0.040821660310029984,\n",
       "  'token': 8503,\n",
       "  'token_str': ' database',\n",
       "  'sequence': 'from database import datetime'},\n",
       " {'score': 0.04064479470252991,\n",
       "  'token': 7127,\n",
       "  'token_str': ' calendar',\n",
       "  'sequence': 'from calendar import datetime'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model=pretrained_file)\n",
    "unmasker(\"from <mask> import datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbcf41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
