{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2395ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d27da2",
   "metadata": {},
   "source": [
    "# 1.训练Glove中文词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0d95e",
   "metadata": {},
   "source": [
    "### 1.1将文本分词并去除停用词后按行写入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdad253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2341964fa4448fbb3f753df6e45ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/501644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d1/4_gsqv2176z583_7rmpm27lh0000gn/T/jieba.cache\n",
      "Loading model cost 0.300 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/THUCNews/train.csv').dropna().reset_index(drop=True)\n",
    "stopwords = [line.strip() for line in open('../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "f = open('./stanford-Glove/THUCNews.txt', 'w')\n",
    "for title in tqdm(df['title']):   \n",
    "    # 去除标点符号\n",
    "    title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "    tokens = [token for token in jieba.cut(title.strip()) if token not in stopwords]\n",
    "    f.write(' '.join(tokens) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc4f7d",
   "metadata": {},
   "source": [
    "### 1.2使用standford/Glove工具训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3441c201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "mkdir -p build\n",
      "\n",
      "$ build/vocab_count -min-count 5 -verbose 2 < THUCNews.txt > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[11G3300000 tokens.\u001b[11G3400000 tokens.\u001b[11G3500000 tokens.\u001b[11G3600000 tokens.\u001b[11G3700000 tokens.\u001b[0GProcessed 3745492 tokens.\n",
      "Counted 192967 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 52133.\n",
      "\n",
      "$ build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < THUCNews.txt > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 52133 words.\n",
      "Building lookup table...table contains 86401563 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[19G3300000\u001b[19G3400000\u001b[19G3500000\u001b[19G3600000\u001b[19G3700000\u001b[0GProcessed 3745492 tokens.\n",
      "Writing cooccurrences to disk.........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[39G7500000 lines.\u001b[39G7600000 lines.\u001b[39G7700000 lines.\u001b[39G7800000 lines.\u001b[39G7900000 lines.\u001b[39G8000000 lines.\u001b[39G8100000 lines.\u001b[39G8200000 lines.\u001b[39G8300000 lines.\u001b[39G8400000 lines.\u001b[39G8500000 lines.\u001b[39G8600000 lines.\u001b[39G8700000 lines.\u001b[39G8800000 lines.\u001b[39G8900000 lines.\u001b[39G9000000 lines.\u001b[39G9100000 lines.\u001b[39G9200000 lines.\u001b[39G9300000 lines.\u001b[39G9400000 lines.\u001b[39G9500000 lines.\u001b[39G9600000 lines.\u001b[39G9700000 lines.\u001b[39G9800000 lines.\u001b[39G9900000 lines.\u001b[39G10000000 lines.\u001b[39G10100000 lines.\u001b[39G10200000 lines.\u001b[39G10300000 lines.\u001b[39G10400000 lines.\u001b[39G10500000 lines.\u001b[39G10600000 lines.\u001b[39G10700000 lines.\u001b[39G10800000 lines.\u001b[39G10900000 lines.\u001b[39G11000000 lines.\u001b[39G11100000 lines.\u001b[39G11200000 lines.\u001b[39G11300000 lines.\u001b[39G11400000 lines.\u001b[39G11500000 lines.\u001b[39G11600000 lines.\u001b[39G11700000 lines.\u001b[39G11800000 lines.\u001b[39G11900000 lines.\u001b[39G12000000 lines.\u001b[39G12100000 lines.\u001b[39G12200000 lines.\u001b[39G12300000 lines.\u001b[39G12400000 lines.\u001b[39G12500000 lines.\u001b[39G12600000 lines.\u001b[39G12700000 lines.\u001b[0GMerging cooccurrence files: processed 12770276 lines.\n",
      "\n",
      "$ build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "Using random seed 1664515925\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 12770276 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G12770276 lines.\u001b[0GMerging temp files: processed 12770276 lines.\n",
      "\n",
      "$ build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 12770276 lines.\n",
      "Initializing parameters...Using random seed 1664515927\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 52133\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "09/30/22 - 01:32.09PM, iter: 001, cost: 0.079658\n",
      "09/30/22 - 01:32.10PM, iter: 002, cost: 0.066233\n",
      "09/30/22 - 01:32.12PM, iter: 003, cost: 0.058172\n",
      "09/30/22 - 01:32.13PM, iter: 004, cost: 0.052132\n",
      "09/30/22 - 01:32.15PM, iter: 005, cost: 0.047218\n",
      "09/30/22 - 01:32.16PM, iter: 006, cost: 0.043467\n",
      "09/30/22 - 01:32.17PM, iter: 007, cost: 0.040689\n",
      "09/30/22 - 01:32.19PM, iter: 008, cost: 0.038606\n",
      "09/30/22 - 01:32.20PM, iter: 009, cost: 0.036999\n",
      "09/30/22 - 01:32.22PM, iter: 010, cost: 0.035739\n",
      "09/30/22 - 01:32.23PM, iter: 011, cost: 0.034719\n",
      "09/30/22 - 01:32.25PM, iter: 012, cost: 0.033885\n",
      "09/30/22 - 01:32.26PM, iter: 013, cost: 0.033173\n",
      "09/30/22 - 01:32.27PM, iter: 014, cost: 0.032576\n",
      "09/30/22 - 01:32.29PM, iter: 015, cost: 0.032063\n"
     ]
    }
   ],
   "source": [
    "# 从github clone斯坦福德glove训练repo\n",
    "# git clone https://github.com/stanfordnlp/GloVe.git\n",
    "# mv GloVe stanford-glove\n",
    "# 把demo.sh中的CORPUS=改成CORPUS=THUCNews.txt\n",
    "\n",
    "!cd stanford-glove && make && sh demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ff9e5",
   "metadata": {},
   "source": [
    "# 2.加载Glove词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea7ef3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52134, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载Glove预训练的词向量\n",
    "embeddings = torchtext.vocab.Vectors(name ='./stanford-glove/vectors.txt')\n",
    "embeddings.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b908eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0794, -1.5969, -0.3386,  0.5643, -0.2867, -0.1013, -0.8174,  1.5820,\n",
      "        -0.5419, -0.0539, -0.6309, -1.0643, -0.1052,  0.0314,  2.1225,  1.0641,\n",
      "        -0.2695, -0.9872, -0.5653, -1.5975,  0.1454,  0.0120,  0.3458,  0.3492,\n",
      "        -0.0757,  1.1362, -1.1327, -0.5551, -1.0531,  1.4729,  0.0657, -1.4755,\n",
      "        -2.0678,  0.5270, -0.9490,  1.6898,  0.4204, -2.2277, -0.3642, -0.6742,\n",
      "        -1.1886,  0.3295,  0.2152, -0.1416, -0.9151,  0.2209,  0.0389, -0.1031,\n",
      "        -0.8291, -0.8683])\n",
      "tensor([-0.3979,  0.1319, -0.3189,  0.5688, -0.4871,  0.7348, -0.2840, -0.2068,\n",
      "        -0.0486, -0.4415, -0.4795,  0.2905, -0.6084, -0.0958, -0.2738, -0.3969,\n",
      "        -0.6119,  0.2108, -0.0191, -0.3205, -0.2233,  0.0657,  0.1794, -0.0613,\n",
      "        -0.4956,  0.3792, -0.0049,  0.0338, -0.1669,  0.4913, -0.6773, -0.1883,\n",
      "         0.5105,  0.1810, -0.7138,  0.0232, -0.5813, -0.1872, -0.4647, -0.6754,\n",
      "         0.3009, -0.1071,  0.3422, -0.5923, -0.1766,  0.0090,  0.6996,  0.2216,\n",
      "         0.4430,  0.7115])\n"
     ]
    }
   ],
   "source": [
    "# 查看词向量\n",
    "print(embeddings.get_vecs_by_tokens('中国'))\n",
    "print(embeddings.get_vecs_by_tokens('自然'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27ac8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1892,  0.0564, -0.9085,  ..., -1.6788,  0.6869, -0.0925],\n",
      "        [-1.8635,  0.6784, -0.2948,  ..., -1.6540,  0.8157, -0.4789],\n",
      "        [ 0.2817,  0.2448, -0.2553,  ..., -0.4893, -0.5367,  0.1656],\n",
      "        ...,\n",
      "        [-0.1367, -0.1138, -0.0513,  ...,  0.1718,  0.0350,  0.0153],\n",
      "        [-0.0685, -0.4505, -0.0600,  ...,  0.3472,  0.0804,  0.1158],\n",
      "        [ 0.0308,  0.0091,  0.0297,  ...,  0.0334, -0.0045,  0.0352]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 将预训练vectors加载到Embdding网络中\n",
    "# freeze为True，则冻结embed层的参数\n",
    "embed = torch.nn.Embedding.from_pretrained(embeddings.vectors, freeze=True)  \n",
    "print(embed.weight)\n",
    "print(embed.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de023f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
