{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b894dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9e8e7",
   "metadata": {},
   "source": [
    "### BERT输入数据构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6062816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDateset(Dataset):\n",
    "    def __init__(self, tokenizer, dateset_path, dateset_type='train', num_sample=1000, max_len=128):\n",
    "        super(MyDateset, self).__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dateset_path, dateset_type + '.csv')).sample(num_sample)\n",
    "\n",
    "        paragraphs = []\n",
    "        for c, f in zip(df['class'], df['file']):\n",
    "            with open(os.path.join(dateset_path, c, f)) as file:\n",
    "                paragraphs.append([sentence for paragraph in file.readlines()\n",
    "                                   for sentence in paragraph.split('。') if sentence.strip()])\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        for paragraph in tqdm(paragraphs):\n",
    "            for i in range(len(paragraph) - 1):\n",
    "                sentence_a = paragraph[i]\n",
    "                # 50%的概率将连续两个句子拼接在一起，50%概率将不相邻的两个句子拼接在一起\n",
    "                if random.random() < 0.5:\n",
    "                    is_next = 1\n",
    "                    sentence_b = paragraph[i + 1]\n",
    "                else:\n",
    "                    sentence_b = random.choice(random.choice(paragraphs))\n",
    "                    is_next = 0\n",
    "\n",
    "                # 将两个句子进行编码\n",
    "                encoded = tokenizer.encode_plus(sentence_a, sentence_b,\n",
    "                                                max_length=max_len, padding='max_length')\n",
    "\n",
    "                # 如果两个句子拼起来超过最大长度，则跳过\n",
    "                if len(encoded['input_ids']) > self.max_len:\n",
    "                    continue\n",
    "\n",
    "                encoded['is_next'] = is_next  # 是否相邻句子标识\n",
    "                encoded = self.get_mlm_data(encoded)  # 进行掩码操作\n",
    "                self.examples.append(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.LongTensor(self.examples[idx].input_ids),\n",
    "            torch.LongTensor(self.examples[idx].token_type_ids),\n",
    "            torch.LongTensor(self.examples[idx].attention_mask),\n",
    "            torch.LongTensor([self.examples[idx].is_next]),\n",
    "            torch.LongTensor(self.examples[idx].pred_positions),\n",
    "            torch.LongTensor(self.examples[idx].pred_labels)\n",
    "        )\n",
    "\n",
    "    def get_mlm_data(self, encoded):\n",
    "        candidate_pred_positions = []  # 除去特殊token外的所有token的位置\n",
    "        for i, token in enumerate(encoded['input_ids']):\n",
    "            # <CLS> <SEP> <PAD> 这三个token不做预测\n",
    "            if token in [self.tokenizer.cls_token_id,\n",
    "                         self.tokenizer.sep_token_id,\n",
    "                         self.tokenizer.pad_token_id]:\n",
    "                continue\n",
    "            candidate_pred_positions.append(i)\n",
    "\n",
    "        # 随机替换15%的token为<MASK>\n",
    "        num_mlm_preds = max(1, round(sum(encoded['attention_mask']) * 0.15))\n",
    "\n",
    "        # 要预测的token的位置\n",
    "        pred_positions = sorted(random.sample(candidate_pred_positions, num_mlm_preds))\n",
    "\n",
    "        # 要预测的token的真实值\n",
    "        pred_labels = [encoded['input_ids'][pos] for pos in pred_positions]\n",
    "\n",
    "        for pos in pred_positions:\n",
    "            if random.random() < 0.8:\n",
    "                # 80%的概率将token替换为<MASK>\n",
    "                encoded['input_ids'][pos] = tokenizer.mask_token_id\n",
    "            else:\n",
    "\n",
    "                if random.random() < 0.5:\n",
    "                    # 10%的概率token不变\n",
    "                    continue\n",
    "                else:\n",
    "                    # 10%的概率随机替换成另外一个token\n",
    "                    encoded['input_ids'][pos] = random.choice(range(106, tokenizer.vocab_size))\n",
    "\n",
    "        # 将要预测的token位置和真实值pad到max_len * 0.15的长度，方便批量计算\n",
    "        max_num_mlm_preds = round(self.max_len * 0.15)\n",
    "        pred_positions += [0] * (max_num_mlm_preds - num_mlm_preds)\n",
    "        pred_labels += [0] * (max_num_mlm_preds - num_mlm_preds)\n",
    "\n",
    "        encoded['pred_positions'] = pred_positions\n",
    "        encoded['pred_labels'] = pred_labels\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fd458",
   "metadata": {},
   "source": [
    "### BERT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bee3f",
   "metadata": {},
   "source": [
    "#### Embdding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59502448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, hidden_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        # token embedding\n",
    "        self.tok_embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        # 两个句子的embedding\n",
    "        self.seg_embed = nn.Embedding(2, hidden_size)\n",
    "        # 位置embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, hidden_size)\n",
    "        # 层归一化\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        # x输入：批量大小 * 步长\n",
    "        seq_len = x.shape[1]\n",
    "        # 位置编码，扩展维度后和x输入一样，批量大小 * 步长\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand_as(x)\n",
    "        pos = pos.to(device)\n",
    "        # 三个embedding相加\n",
    "        embedded = self.tok_embed(x) + self.seg_embed(seg) + self.pos_embed(pos)\n",
    "        return self.norm(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6234bb2",
   "metadata": {},
   "source": [
    "#### 注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缩放点积注意力\n",
    "class ScaledDotProductionAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductionAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask):\n",
    "        # query/key/value：批量大小 * 头数 * 步长 * 向量维度\n",
    "        # attn_mask尺寸：批量大小 * 头数 * 步长 * 步长\n",
    "        d_k = key.shape[-1]  # key的维度\n",
    "        # Q * K转置 / 根号d_k，scores尺寸：批量大小 * 头数 * 步长 * 步长\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn_weights = self.softmax(scores)\n",
    "        return torch.matmul(attn_weights, value)  # 返回的结果尺寸：批量大小 * 头数 * 步长 * 向量维度\n",
    "\n",
    "\n",
    "# 多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 输入和输出的维度\n",
    "        self.num_heads = num_heads  # 头数\n",
    "        self.key_size = self.value_size = self.hidden_size // self.num_heads  # 输入输出维度必须可以整除头数，方便并行\n",
    "        self.attention = ScaledDotProductionAttention()  # 缩放点积注意力\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)  # Q的投影参数\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)  # K的投影参数\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)  # V的投影参数\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)  # 全连接层，多头分开做自注意力后，再拼接起来，接一个全连接层\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask):\n",
    "        # Q K V输入尺寸：批量大小 * 步长 * 维度\n",
    "        # mask输入尺寸：批量大小 * 步长 * 步长\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "\n",
    "        # 方便多头并行计算，QKV投影后reshape成 批量大小 * 步长 * 头数 * 维度，再交换1、2维度，变成 批量大小 * 头数 * 步长 * 维度\n",
    "        q_s = self.W_Q(query).reshape(batch_size, -1, self.num_heads, self.key_size).transpose(1, 2)\n",
    "        k_s = self.W_Q(key).reshape(batch_size, -1, self.num_heads, self.key_size).transpose(1, 2)\n",
    "        v_s = self.W_Q(value).reshape(batch_size, -1, self.num_heads, self.value_size).transpose(1, 2)\n",
    "\n",
    "        # mask处理成 批量大小 * 头数 * 步长 * 步长\n",
    "        attn_mask = attn_mask.data.eq(0).unsqueeze(1).expand(batch_size, seq_len, seq_len)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "        # context尺寸：批量大小 * 头数 * 步长 * 单个头的维度\n",
    "        context = self.attention(q_s, k_s, v_s, attn_mask)\n",
    "\n",
    "        # context尺寸：批量大小 * 步长 * hidden_size\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        output = self.fc(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a7445",
   "metadata": {},
   "source": [
    "#### 前馈网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5cc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size):\n",
    "        # 两个全连接层，使用gelu作为激活函数\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, ffn_size)\n",
    "        self.fc2 = nn.Linear(ffn_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c11d2",
   "metadata": {},
   "source": [
    "#### 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d253c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        # 层归一化 + 残差连接\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(norm_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.layer_norm(x + self.dropout(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d2b5f",
   "metadata": {},
   "source": [
    "#### Transformer Encoder块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e9e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size, ffn_size, dropout):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.add_norm1 = AddNorm(norm_shape=hidden_size, dropout=dropout)\n",
    "        self.ffn = PositionWiseFFN(hidden_size=hidden_size, ffn_size=ffn_size)\n",
    "        self.add_norm2 = AddNorm(norm_shape=hidden_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        output = self.add_norm1(x, self.attention(x, x, x, attn_mask))\n",
    "        output = self.add_norm2(output, self.ffn(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6de258",
   "metadata": {},
   "source": [
    "#### BERT模型，是否相邻句子+MASK位置预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ba072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, max_len, hidden_size, ffn_size, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(vocab_size=vocab_size, max_len=max_len, hidden_size=hidden_size)\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.layers.add_module(f'{i}', EncoderBlock(num_heads=num_heads, hidden_size=hidden_size,\n",
    "                                                        ffn_size=ffn_size, dropout=dropout))\n",
    "\n",
    "        # 是否下一个句子预测\n",
    "        self.is_next_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        # 预测mask掉的token\n",
    "        self.mask_lm = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, attn_mask, pred_positions):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, attn_mask)\n",
    "\n",
    "        # 用输出的第一个位置，即[CLS]的768维的向量表示，拿来做是否是相邻句子的预测\n",
    "        cls_output = output[:, 0]\n",
    "        logit_is_next = self.is_next_classifier(cls_output)\n",
    "\n",
    "        # [MASK]位置的预测\n",
    "        batch_size, num_pred_positions = pred_positions.shape\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        \n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        batch_idx = batch_idx.to(device)\n",
    "        \n",
    "        mask_output = output[batch_idx, pred_positions]\n",
    "        mask_output = mask_output.reshape((batch_size, num_pred_positions, -1))\n",
    "        logit_mask = self.mask_lm(mask_output)\n",
    "        \n",
    "        return logit_is_next, logit_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e1d6f",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46b50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "max_len = 128\n",
    "hidden_size = 768\n",
    "ffn_size = 768\n",
    "dropout = 0.2\n",
    "lr = 1e-4\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../../models/bert-base-chinese/')\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1a6c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e226e30a2ded4000ad3b32bde6c44a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dateset_path = '../../datasets/THUCNews'\n",
    "dataset_type = 'train'\n",
    "\n",
    "train_dataset = MyDateset(tokenizer, dateset_path, dataset_type, num_sample=1000)\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e2b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76317322\n"
     ]
    }
   ],
   "source": [
    "model = BERT(num_layers=num_layers, num_heads=num_heads, vocab_size=vocab_size,\n",
    "             max_len=max_len, hidden_size=hidden_size, ffn_size=ffn_size, dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# 模型可学习参数量\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b0b238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1|Train Loss: 0.0784\n",
      "Epochs:2|Train Loss: 0.0721\n",
      "Epochs:3|Train Loss: 0.0718\n",
      "Epochs:4|Train Loss: 0.0715\n",
      "Epochs:5|Train Loss: 0.0706\n",
      "Epochs:6|Train Loss: 0.0691\n",
      "Epochs:7|Train Loss: 0.0677\n",
      "Epochs:8|Train Loss: 0.0664\n",
      "Epochs:9|Train Loss: 0.0650\n",
      "Epochs:10|Train Loss: 0.0636\n",
      "Epochs:11|Train Loss: 0.0622\n",
      "Epochs:12|Train Loss: 0.0610\n",
      "Epochs:13|Train Loss: 0.0596\n",
      "Epochs:14|Train Loss: 0.0583\n",
      "Epochs:15|Train Loss: 0.0568\n",
      "Epochs:16|Train Loss: 0.0552\n",
      "Epochs:17|Train Loss: 0.0537\n",
      "Epochs:18|Train Loss: 0.0521\n",
      "Epochs:19|Train Loss: 0.0509\n",
      "Epochs:20|Train Loss: 0.0499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss_train = 0\n",
    "    for input_ids, segment_ids, attn_mask, is_next, pred_positions, pred_labels in data_loader:\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        is_next = is_next.to(device)\n",
    "        pred_positions = pred_positions.to(device)\n",
    "        pred_labels = pred_labels.to(device)\n",
    "\n",
    "        logit_is_next, logit_mask = model(input_ids, segment_ids, attn_mask, pred_positions)\n",
    "\n",
    "        loss_is_next = criterion(logit_is_next, is_next.view(-1))\n",
    "\n",
    "        loss_mask = criterion(logit_mask.view(-1, vocab_size), pred_labels.view(-1))\n",
    "\n",
    "        loss = loss_is_next + loss_mask\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epochs:{epoch + 1}|Train Loss:{total_loss_train / len(train_dataset): .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3c002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
