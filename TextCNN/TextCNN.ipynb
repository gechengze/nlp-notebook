{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2b39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b60ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, max_len, debug=True):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "        df = df.dropna()\n",
    "        if debug:\n",
    "            df = df.sample(2000).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(50000).reset_index(drop=True)\n",
    "        # 读取常用停用词\n",
    "        stopwords = [line.strip() for line in open('../../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "        sentences = []\n",
    "        for title in df['title']:\n",
    "            # 去除标点符号\n",
    "            title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "            # jieba分词\n",
    "            sentence_seged = jieba.cut(title.strip())\n",
    "            outstr = ''\n",
    "            for word in sentence_seged:\n",
    "                if word != '\\t' and word not in stopwords:\n",
    "                    outstr += word\n",
    "                    outstr += ' '\n",
    "            if outstr != '':\n",
    "                sentences.append(outstr)\n",
    "        # 获取所有词(token), <pad>用来填充不满足max_len的句子\n",
    "        token_list = ['<pad>'] + list(set(' '.join(sentences).split()))\n",
    "        # token和index互转字典\n",
    "        self.token2idx = {token: i for i, token in enumerate(token_list)}\n",
    "        self.idx2token = {i: token for i, token in enumerate(token_list)}\n",
    "        self.vocab_size = len(self.token2idx)\n",
    "\n",
    "        self.inputs = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            input_ = [self.token2idx[token] for token in tokens]\n",
    "            if len(input_) < max_len:\n",
    "                self.inputs.append(input_ + [self.token2idx['<pad>']] * (max_len - len(input_)))\n",
    "            else:\n",
    "                self.inputs.append(input_[: max_len])\n",
    "\n",
    "        self.labels = [[label] for label in df['label'].values.tolist()]\n",
    "        self.n_class = len(df['label'].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.inputs[idx]), torch.LongTensor(self.labels[idx])\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_class):\n",
    "        super().__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 输入通道为1，卷成16通道输出，卷积核大小为(3*embed_size)，3类似于n-gram，可以换\n",
    "        self.conv = nn.Conv2d(1, 16, (3, embed_size))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # 输出头\n",
    "        self.fc = nn.Linear(16, n_class)\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size * 句子长度]\n",
    "        x = self.embedding(x)  # [batch_size * 句子长度 * embed_size]\n",
    "        x = x.unsqueeze(1)  # [batch_size * 1 * 句子长度 * embed_size]，加一个维度，用于卷积层的输入\n",
    "        x = self.conv(x)  # [batch_size * 16(卷积层输出通道数) * 8(卷积后的宽) * 1(卷积后的高)]\n",
    "        x = x.squeeze(3)  # [batch_size * 16(卷积层输出通道数) * 8(卷积后的宽)] 压缩大小为1的维度\n",
    "        x = torch.relu(x)  # 激活函数，尺寸不变\n",
    "        x = torch.max_pool1d(x, x.size(2))  # 在每个通道做最大池化，[batch_size * 16(卷积层输出通道数) * 1]\n",
    "        x = x.squeeze(2)  # 压缩维度2，[batch_size * 16(卷积层输出通道数)]\n",
    "        x = self.dropout(x)  # dropout，尺寸不变\n",
    "        logits = self.fc(x)  # 全连接输出头，[batch_size * n_class]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b19d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/d1/4_gsqv2176z583_7rmpm27lh0000gn/T/jieba.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 2.0988645553588867\n",
      "epoch: 2 , loss: 1.6732847690582275\n",
      "epoch: 3 , loss: 1.7872973680496216\n",
      "epoch: 4 , loss: 1.632114291191101\n",
      "epoch: 5 , loss: 2.034989833831787\n",
      "epoch: 6 , loss: 1.2769547700881958\n",
      "epoch: 7 , loss: 0.8881691694259644\n",
      "epoch: 8 , loss: 0.7390443682670593\n",
      "epoch: 9 , loss: 0.9840537905693054\n",
      "epoch: 10 , loss: 0.6907981634140015\n",
      "epoch: 11 , loss: 0.8755264282226562\n",
      "epoch: 12 , loss: 0.47071027755737305\n",
      "epoch: 13 , loss: 0.2673209309577942\n",
      "epoch: 14 , loss: 0.4598577320575714\n",
      "epoch: 15 , loss: 0.3407303988933563\n",
      "epoch: 16 , loss: 0.4497127830982208\n",
      "epoch: 17 , loss: 0.10531115531921387\n",
      "epoch: 18 , loss: 0.37725019454956055\n",
      "epoch: 19 , loss: 0.4173489809036255\n",
      "epoch: 20 , loss: 0.2735407054424286\n",
      "epoch: 21 , loss: 0.11588075011968613\n",
      "epoch: 22 , loss: 0.13563832640647888\n",
      "epoch: 23 , loss: 0.07552756369113922\n",
      "epoch: 24 , loss: 0.10603988170623779\n",
      "epoch: 25 , loss: 0.23832593858242035\n",
      "epoch: 26 , loss: 0.04341195523738861\n",
      "epoch: 27 , loss: 0.20184533298015594\n",
      "epoch: 28 , loss: 0.10441288352012634\n",
      "epoch: 29 , loss: 0.09312985092401505\n",
      "epoch: 30 , loss: 0.08736424148082733\n",
      "epoch: 31 , loss: 0.23486639559268951\n",
      "epoch: 32 , loss: 0.011024708859622478\n",
      "epoch: 33 , loss: 0.1881861537694931\n",
      "epoch: 34 , loss: 0.07356742769479752\n",
      "epoch: 35 , loss: 0.01652516797184944\n",
      "epoch: 36 , loss: 0.037907421588897705\n",
      "epoch: 37 , loss: 0.02686919830739498\n",
      "epoch: 38 , loss: 0.08193457871675491\n",
      "epoch: 39 , loss: 0.03270254284143448\n",
      "epoch: 40 , loss: 0.010941896587610245\n",
      "epoch: 41 , loss: 0.12579034268856049\n",
      "epoch: 42 , loss: 0.050476714968681335\n",
      "epoch: 43 , loss: 0.08512263000011444\n",
      "epoch: 44 , loss: 0.0944894328713417\n",
      "epoch: 45 , loss: 0.08125633001327515\n",
      "epoch: 46 , loss: 0.029051648452878\n",
      "epoch: 47 , loss: 0.0073861368000507355\n",
      "epoch: 48 , loss: 0.007992567494511604\n",
      "epoch: 49 , loss: 0.11307477205991745\n",
      "epoch: 50 , loss: 0.03962830826640129\n",
      "epoch: 51 , loss: 0.0013953896705061197\n",
      "epoch: 52 , loss: 0.01879403367638588\n",
      "epoch: 53 , loss: 0.0552964024245739\n",
      "epoch: 54 , loss: 0.011445942334830761\n",
      "epoch: 55 , loss: 0.004962654318660498\n",
      "epoch: 56 , loss: 0.02250526286661625\n",
      "epoch: 57 , loss: 0.046904340386390686\n",
      "epoch: 58 , loss: 0.08418771624565125\n",
      "epoch: 59 , loss: 0.021523457020521164\n",
      "epoch: 60 , loss: 0.08636060357093811\n",
      "epoch: 61 , loss: 0.005336354952305555\n",
      "epoch: 62 , loss: 0.014758560806512833\n",
      "epoch: 63 , loss: 0.02627008967101574\n",
      "epoch: 64 , loss: 0.037708528339862823\n",
      "epoch: 65 , loss: 0.00412113917991519\n",
      "epoch: 66 , loss: 0.05168202891945839\n",
      "epoch: 67 , loss: 0.011379721574485302\n",
      "epoch: 68 , loss: 0.004837930202484131\n",
      "epoch: 69 , loss: 0.0012761485995724797\n",
      "epoch: 70 , loss: 0.025436345487833023\n",
      "epoch: 71 , loss: 0.08312883973121643\n",
      "epoch: 72 , loss: 0.0831339880824089\n",
      "epoch: 73 , loss: 0.15487031638622284\n",
      "epoch: 74 , loss: 0.06400782614946365\n",
      "epoch: 75 , loss: 0.02646801806986332\n",
      "epoch: 76 , loss: 0.002494032261893153\n",
      "epoch: 77 , loss: 0.001271929475478828\n",
      "epoch: 78 , loss: 0.005241514183580875\n",
      "epoch: 79 , loss: 0.031080098822712898\n",
      "epoch: 80 , loss: 0.02328745275735855\n",
      "epoch: 81 , loss: 0.013475262559950352\n",
      "epoch: 82 , loss: 0.00274881673976779\n",
      "epoch: 83 , loss: 0.011174953542649746\n",
      "epoch: 84 , loss: 0.19902414083480835\n",
      "epoch: 85 , loss: 0.17371739447116852\n",
      "epoch: 86 , loss: 0.00567785557359457\n",
      "epoch: 87 , loss: 0.0008357019396498799\n",
      "epoch: 88 , loss: 0.02480960637331009\n",
      "epoch: 89 , loss: 0.005465076304972172\n",
      "epoch: 90 , loss: 0.044302213937044144\n",
      "epoch: 91 , loss: 0.016008153557777405\n",
      "epoch: 92 , loss: 0.037888672202825546\n",
      "epoch: 93 , loss: 0.04694010689854622\n",
      "epoch: 94 , loss: 0.01708764024078846\n",
      "epoch: 95 , loss: 7.375329732894897e-05\n",
      "epoch: 96 , loss: 0.0008732432033866644\n",
      "epoch: 97 , loss: 0.018604403361678123\n",
      "epoch: 98 , loss: 0.0011304221116006374\n",
      "epoch: 99 , loss: 0.000516515108756721\n",
      "epoch: 100 , loss: 0.08295240998268127\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(max_len=10)  # 构造长度为10的句子输入，超过10的句子切掉，不足10的补<pad>\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "model = TextCNN(vocab_size=dataset.vocab_size, embed_size=128, n_class=dataset.n_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(100):\n",
    "    for feature, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feature)\n",
    "        loss = criterion(logits, target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch:', epoch + 1, ', loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e769d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "盘整 改向 趋势 <pad> <pad> <pad> <pad> <pad> <pad> <pad> ---> label: 股票\n",
      "地量 频出 面对 <pad> <pad> <pad> <pad> <pad> <pad> <pad> ---> label: 股票\n",
      "低 库存 国际 粮价 持续 上涨 动力 <pad> <pad> <pad> ---> label: 财经\n",
      "快乐 男声 曝 黑幕 疑云 传 刘心 第五名 已定 图 ---> label: 娱乐\n",
      "李孝利 月 中 发行 第张 专辑 三度 涉嫌 抄袭 图 ---> label: 家居\n",
      "赫斯特 国际 业务 发起 亿美元 要约 <pad> <pad> <pad> <pad> ---> label: 股票\n",
      "谷歌风 投向 付费 搜索 服务 投资 万美元 <pad> <pad> <pad> ---> label: 科技\n",
      "上海 证监局 部署 今年 期货 监管 十大 工作 <pad> <pad> ---> label: 财经\n",
      "种 装修 风格 助 打造 个性 客厅 组图 <pad> <pad> ---> label: 家居\n",
      "打工仔 冒充 美女 网上 诈骗 初中 女生 万 <pad> <pad> ---> label: 社会\n",
      "廖斌 谈 做好 网站 需要 强悍 身体 <pad> <pad> <pad> ---> label: 科技\n",
      "零碳 家庭 倡议书 <pad> <pad> <pad> <pad> <pad> <pad> <pad> ---> label: 家居\n",
      "限量 台徕 卡 发布 版 <pad> <pad> <pad> <pad> <pad> ---> label: 科技\n",
      "性能 出众 单反 辅 机理 光送 卡 <pad> <pad> <pad> ---> label: 科技\n",
      "大学 设置 拆迁 专业 解 不了 不公 <pad> <pad> <pad> ---> label: 教育\n",
      "大厦 凌晨 起火 保安 爬层 逐户 叫醒 居民 <pad> <pad> ---> label: 社会\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "predict = model(feature).max(1)[1].tolist()\n",
    "for i in range(len(feature.tolist())):\n",
    "    print(' '.join([dataset.idx2token[idx] for idx in feature.tolist()[i]]),\n",
    "          '---> label:',\n",
    "          dict(zip(df['label'], df['class']))[predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25031b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
