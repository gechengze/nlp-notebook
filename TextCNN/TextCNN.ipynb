{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2b39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b60ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, max_len, debug=True):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "        df = df.dropna()\n",
    "        if debug:\n",
    "            df = df.sample(2000).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(50000).reset_index(drop=True)\n",
    "        # 读取常用停用词\n",
    "        stopwords = [line.strip() for line in open('../../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "        sentences = []\n",
    "        for title in df['title']:\n",
    "            # 去除标点符号\n",
    "            title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "            # jieba分词\n",
    "            sentence_seged = jieba.cut(title.strip())\n",
    "            outstr = ''\n",
    "            for word in sentence_seged:\n",
    "                if word != '\\t' and word not in stopwords:\n",
    "                    outstr += word\n",
    "                    outstr += ' '\n",
    "            if outstr != '':\n",
    "                sentences.append(outstr)\n",
    "        # 获取所有词(token), <pad>用来填充不满足max_len的句子\n",
    "        token_list = ['<pad>'] + list(set(' '.join(sentences).split()))\n",
    "        # token和index互转字典\n",
    "        self.token2idx = {token: i for i, token in enumerate(token_list)}\n",
    "        self.idx2token = {i: token for i, token in enumerate(token_list)}\n",
    "        self.vocab_size = len(self.token2idx)\n",
    "\n",
    "        self.inputs = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            input_ = [self.token2idx[token] for token in tokens]\n",
    "            if len(input_) < max_len:\n",
    "                self.inputs.append(input_ + [self.token2idx['<pad>']] * (max_len - len(input_)))\n",
    "            else:\n",
    "                self.inputs.append(input_[: max_len])\n",
    "\n",
    "        self.labels = [[label] for label in df['label'].values.tolist()]\n",
    "        self.n_class = len(df['label'].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.inputs[idx]), torch.LongTensor(self.labels[idx])\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_class):\n",
    "        super().__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 输入通道为1，卷成16通道输出，卷积核大小为(3*embed_size)，3类似于n-gram，可以换\n",
    "        self.conv = nn.Conv2d(1, 16, (3, embed_size))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # 输出头\n",
    "        self.fc = nn.Linear(16, n_class)\n",
    "\n",
    "    def forward(self, x):  # x: [batch_size * 句子长度]\n",
    "        x = self.embedding(x)  # [batch_size * 句子长度 * embed_size]\n",
    "        x = x.unsqueeze(1)  # [batch_size * 1 * 句子长度 * embed_size]，加一个维度，用于卷积层的输入\n",
    "        x = self.conv(x)  # [batch_size * 16(卷积层输出通道数) * 8(卷积后的宽) * 1(卷积后的高)]\n",
    "        x = x.squeeze(3)  # [batch_size * 16(卷积层输出通道数) * 8(卷积后的宽)] 压缩大小为1的维度\n",
    "        x = torch.relu(x)  # 激活函数，尺寸不变\n",
    "        x = torch.max_pool1d(x, x.size(2))  # 在每个通道做最大池化，[batch_size * 16(卷积层输出通道数) * 1]\n",
    "        x = x.squeeze(2)  # 压缩维度2，[batch_size * 16(卷积层输出通道数)]\n",
    "        x = self.dropout(x)  # dropout，尺寸不变\n",
    "        logits = self.fc(x)  # 全连接输出头，[batch_size * n_class]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b19d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d1/4_gsqv2176z583_7rmpm27lh0000gn/T/jieba.cache\n",
      "Loading model cost 0.403 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 2.773195743560791\n",
      "epoch: 2 , loss: 1.8674651384353638\n",
      "epoch: 3 , loss: 1.483340859413147\n",
      "epoch: 4 , loss: 1.7299542427062988\n",
      "epoch: 5 , loss: 1.6403207778930664\n",
      "epoch: 6 , loss: 0.9790477752685547\n",
      "epoch: 7 , loss: 1.2651853561401367\n",
      "epoch: 8 , loss: 0.9305796027183533\n",
      "epoch: 9 , loss: 0.6754975318908691\n",
      "epoch: 10 , loss: 0.657089114189148\n",
      "epoch: 11 , loss: 0.7448533773422241\n",
      "epoch: 12 , loss: 0.6199911832809448\n",
      "epoch: 13 , loss: 0.7406424880027771\n",
      "epoch: 14 , loss: 0.3471713960170746\n",
      "epoch: 15 , loss: 0.3989872932434082\n",
      "epoch: 16 , loss: 0.2998690605163574\n",
      "epoch: 17 , loss: 0.21135176718235016\n",
      "epoch: 18 , loss: 0.6030945181846619\n",
      "epoch: 19 , loss: 0.10659075528383255\n",
      "epoch: 20 , loss: 0.23190981149673462\n",
      "epoch: 21 , loss: 0.05693645030260086\n",
      "epoch: 22 , loss: 0.14458638429641724\n",
      "epoch: 23 , loss: 0.2682003676891327\n",
      "epoch: 24 , loss: 0.41655316948890686\n",
      "epoch: 25 , loss: 0.1977047473192215\n",
      "epoch: 26 , loss: 0.06505148857831955\n",
      "epoch: 27 , loss: 0.012891586869955063\n",
      "epoch: 28 , loss: 0.09815986454486847\n",
      "epoch: 29 , loss: 0.10779955983161926\n",
      "epoch: 30 , loss: 0.11766104400157928\n",
      "epoch: 31 , loss: 0.16658605635166168\n",
      "epoch: 32 , loss: 0.1248503029346466\n",
      "epoch: 33 , loss: 0.029117217287421227\n",
      "epoch: 34 , loss: 0.03002508543431759\n",
      "epoch: 35 , loss: 0.01583990640938282\n",
      "epoch: 36 , loss: 0.01749483123421669\n",
      "epoch: 37 , loss: 0.007545290980488062\n",
      "epoch: 38 , loss: 0.055362872779369354\n",
      "epoch: 39 , loss: 0.04861106723546982\n",
      "epoch: 40 , loss: 0.11690451949834824\n",
      "epoch: 41 , loss: 0.14646081626415253\n",
      "epoch: 42 , loss: 0.037940990179777145\n",
      "epoch: 43 , loss: 0.03215443342924118\n",
      "epoch: 44 , loss: 0.1585850715637207\n",
      "epoch: 45 , loss: 0.04779992252588272\n",
      "epoch: 46 , loss: 0.13087135553359985\n",
      "epoch: 47 , loss: 0.026212284341454506\n",
      "epoch: 48 , loss: 0.032181158661842346\n",
      "epoch: 49 , loss: 0.042439982295036316\n",
      "epoch: 50 , loss: 0.04828186333179474\n",
      "epoch: 51 , loss: 0.014944499358534813\n",
      "epoch: 52 , loss: 0.06012532487511635\n",
      "epoch: 53 , loss: 0.053633615374565125\n",
      "epoch: 54 , loss: 0.01002102717757225\n",
      "epoch: 55 , loss: 0.007247967179864645\n",
      "epoch: 56 , loss: 0.17462581396102905\n",
      "epoch: 57 , loss: 0.004751310218125582\n",
      "epoch: 58 , loss: 0.21341200172901154\n",
      "epoch: 59 , loss: 0.003946583718061447\n",
      "epoch: 60 , loss: 0.051726192235946655\n",
      "epoch: 61 , loss: 0.00832673255354166\n",
      "epoch: 62 , loss: 0.050893671810626984\n",
      "epoch: 63 , loss: 0.056452203541994095\n",
      "epoch: 64 , loss: 0.013029176741838455\n",
      "epoch: 65 , loss: 0.08201664686203003\n",
      "epoch: 66 , loss: 0.05019383132457733\n",
      "epoch: 67 , loss: 0.013587385416030884\n",
      "epoch: 68 , loss: 0.0015156576409935951\n",
      "epoch: 69 , loss: 0.0015125030186027288\n",
      "epoch: 70 , loss: 0.05963575094938278\n",
      "epoch: 71 , loss: 0.06554662436246872\n",
      "epoch: 72 , loss: 0.0006834751693531871\n",
      "epoch: 73 , loss: 0.08893818408250809\n",
      "epoch: 74 , loss: 0.0001669566408963874\n",
      "epoch: 75 , loss: 0.004202215000987053\n",
      "epoch: 76 , loss: 0.18674665689468384\n",
      "epoch: 77 , loss: 0.1269092559814453\n",
      "epoch: 78 , loss: 0.019032564014196396\n",
      "epoch: 79 , loss: 0.0312034972012043\n",
      "epoch: 80 , loss: 0.011777987703680992\n",
      "epoch: 81 , loss: 0.2328161597251892\n",
      "epoch: 82 , loss: 0.046914905309677124\n",
      "epoch: 83 , loss: 0.00814696867018938\n",
      "epoch: 84 , loss: 0.004447861108928919\n",
      "epoch: 85 , loss: 0.06761334091424942\n",
      "epoch: 86 , loss: 0.0010052333818748593\n",
      "epoch: 87 , loss: 0.0018791970796883106\n",
      "epoch: 88 , loss: 0.0012500571319833398\n",
      "epoch: 89 , loss: 0.15007668733596802\n",
      "epoch: 90 , loss: 0.0029948095325380564\n",
      "epoch: 91 , loss: 0.0007503366796299815\n",
      "epoch: 92 , loss: 0.0005970581551082432\n",
      "epoch: 93 , loss: 0.0030119719449430704\n",
      "epoch: 94 , loss: 0.027605295181274414\n",
      "epoch: 95 , loss: 0.000543166243005544\n",
      "epoch: 96 , loss: 0.02213035710155964\n",
      "epoch: 97 , loss: 0.033296599984169006\n",
      "epoch: 98 , loss: 0.027505017817020416\n",
      "epoch: 99 , loss: 0.0012612263672053814\n",
      "epoch: 100 , loss: 0.027610279619693756\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(max_len=10)  # 构造长度为10的句子输入，超过10的句子切掉，不足10的补<pad>\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "model = TextCNN(vocab_size=dataset.vocab_size, embed_size=128, n_class=dataset.n_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(100):\n",
    "    for feature, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feature)\n",
    "        loss = criterion(logits, target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch:', epoch + 1, ', loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e769d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奥巴马 获赠 成人 游戏 此类 已成 赢利 重点 <pad> <pad> ---> label: 游戏\n",
      "三 小时 苦练 阿联 意犹未尽 队友 教练 离开 加练 <pad> ---> label: 体育\n",
      "连跌 七天 年 最后 一个 交易日 期盼 惊喜 <pad> <pad> ---> label: 股票\n",
      "业巡 精英 比洞 赛次 轮 彭婕 爆冷 出局 黄 永乐 ---> label: 体育\n",
      "欧文 现身 曼联 基地 接受 体检 加盟 将成 定局 <pad> ---> label: 体育\n",
      "顶级 变焦 牛头 佳能 赠镜 <pad> <pad> <pad> <pad> <pad> ---> label: 科技\n",
      "广东 年 高考 录取 启动 提前 批 实行 平行 志愿 ---> label: 教育\n",
      "新 托福 五大 特点 题型 应对 方法 <pad> <pad> <pad> ---> label: 教育\n",
      "快讯 西飞 国际 尾盘 继续 诡异 狂 拉升 <pad> <pad> ---> label: 股票\n",
      "韩国 决定 年 批量生产 电动汽车 <pad> <pad> <pad> <pad> <pad> ---> label: 时政\n",
      "康健 平保 旗下 授 认股权 <pad> <pad> <pad> <pad> <pad> ---> label: 股票\n",
      "安理会 决定 召开 紧急会议 讨论 朝鲜 核试验 <pad> <pad> <pad> ---> label: 时政\n",
      "股指 回补 跳空 缺口 关注 后续 量 <pad> <pad> <pad> ---> label: 股票\n",
      "环保部 今年 未 批复 环评 项目 涉及 亿 <pad> <pad> ---> label: 时政\n",
      "百强 家具 新品 奢华 实 木家具 <pad> <pad> <pad> <pad> ---> label: 家居\n",
      "探索 七界 万王 全球 热恋 醉汉 进化 <pad> <pad> <pad> ---> label: 游戏\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "predict = model(feature).max(1)[1].tolist()\n",
    "for i in range(len(feature.tolist())):\n",
    "    print(' '.join([dataset.idx2token[idx] for idx in feature.tolist()[i]]),\n",
    "          '---> label:',\n",
    "          dict(zip(df['label'], df['class']))[predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25031b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
