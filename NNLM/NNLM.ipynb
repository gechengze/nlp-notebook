{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da45dec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19250b3e",
   "metadata": {},
   "source": [
    "# 1.准备数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47df5b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, stopwords, debug=True):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        if debug:\n",
    "            df = df.sample(2000).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(50000).reset_index(drop=True)\n",
    "        # 读取常用停用词\n",
    "        counter = Counter()\n",
    "        sentences = []\n",
    "        for title in tqdm(df['title']):   \n",
    "            # 去除标点符号\n",
    "            title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "            tokens = [token for token in tokenizer(title.strip()) if token not in stopwords]\n",
    "            counter.update(tokens)\n",
    "            sentences.append(tokens)\n",
    "        self.vocab = torchtext.vocab.vocab(counter, specials=['<unk>', '<pad>'])\n",
    "        \n",
    "        # 构造输入和输出，输入是每三个词，输出是这三个词的下一个词，也就是简单的n-gram语言模型（n=3）\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for sen in sentences:\n",
    "            for i in range(len(sen) - 3):\n",
    "                self.inputs.append(self.vocab.lookup_indices(sen[i: i + 3]))\n",
    "                self.labels.append([self.vocab[sen[i + 3]]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回一个x和一个y\n",
    "        return torch.LongTensor(self.inputs[idx]), torch.LongTensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b70003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08caffbee3d248109f503489bd89ecd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = '../data/THUCNews/train.csv'\n",
    "debug = True\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='zh_core_web_sm')\n",
    "stopwords = [line.strip() for line in open('../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "dataset = MyDataset(file_path, tokenizer, stopwords)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b21d9",
   "metadata": {},
   "source": [
    "# 2.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08065633",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, n_step, n_hidden):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.n_step = n_step\n",
    "        # vocab size投影到到embed size的空间中\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 构造一个隐藏层，输入大小为 步长 * embed size，输入大小为n_hidden\n",
    "        self.linear = nn.Linear(n_step * embed_size, n_hidden)\n",
    "        # 将n_hidden投影回vocab size大小\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = X.view(-1, self.n_step * self.embed_size)\n",
    "        X = self.linear(X)\n",
    "        X = torch.tanh(X)\n",
    "        y = self.output(X)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd74330",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNLM(\n",
      "  (embedding): Embedding(8358, 256)\n",
      "  (linear): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=8358, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = NNLM(vocab_size=len(dataset.vocab), embed_size=256, n_step=3, n_hidden=256)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 查看模型\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01238105",
   "metadata": {},
   "source": [
    "# 3.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90246afc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss = 9.084500\n",
      "epoch: 2 loss = 7.712342\n",
      "epoch: 3 loss = 6.277411\n",
      "epoch: 4 loss = 4.700910\n",
      "epoch: 5 loss = 3.170568\n",
      "epoch: 6 loss = 1.630075\n",
      "epoch: 7 loss = 0.590093\n",
      "epoch: 8 loss = 0.287548\n",
      "epoch: 9 loss = 0.189612\n",
      "epoch: 10 loss = 0.099930\n",
      "epoch: 11 loss = 0.081222\n",
      "epoch: 12 loss = 0.066866\n",
      "epoch: 13 loss = 0.052047\n",
      "epoch: 14 loss = 0.042389\n",
      "epoch: 15 loss = 0.037109\n",
      "epoch: 16 loss = 0.029919\n",
      "epoch: 17 loss = 0.028010\n",
      "epoch: 18 loss = 0.023675\n",
      "epoch: 19 loss = 0.022178\n",
      "epoch: 20 loss = 0.018222\n"
     ]
    }
   ],
   "source": [
    "# 训练20个epoch\n",
    "for epoch in range(20):\n",
    "    for train_input, train_label in dataloader:\n",
    "        output = model(train_input)\n",
    "        loss = criterion(output, train_label.squeeze_())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch:', epoch + 1, 'loss =', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8e9da",
   "metadata": {},
   "source": [
    "# 4.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b550cb4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "市场 跷 跷板 -> 效应\n",
      "膝伤 欧冠 恐缺 -> 战马赛\n",
      "巴顿 红 牛距 -> 数\n",
      "塞班诺基亚 推亿 塞班 -> 手机\n",
      "危机 冲击 传统 -> 教材\n",
      "年 河北 秦皇岛 -> 中考\n",
      "创业板 上市 门槛 -> 提高\n",
      "皇马 巴萨大乱 战 -> 前因后果\n",
      "突破 转播 北美 -> 落地\n",
      "阿布 亲临 前线 -> 留特里德罗巴\n",
      "项目 涉及 金额 -> 超亿\n",
      "原 高管 退位 -> 兵装摩\n",
      "庆生 王心 凌送 -> 巨乳\n",
      "夫妇 称忠于 良知 -> 代母\n",
      "刺激 资金 进一步 -> 回流\n",
      "仅 需元 东芝 -> 英寸\n",
      "吴雨霏 邓丽欣 再次 -> 合作\n",
      "消息 利好 美股 -> 走强\n",
      "洁欢 乐购 佛山站 -> 盛大\n",
      "几率 达 五成 -> 排除\n",
      "许志安 新年 断 -> 懒根\n",
      "季节 交替 易 -> 敏春季\n",
      "改革 法案 争议 -> 中\n",
      "赵柯 写 真照 -> 曝光\n",
      "切尔西盯 防梅西 候选 -> 曝光\n",
      "次 尿检 大麻 -> 案件\n",
      "发生 里 氏级 -> 地震\n",
      "排名 揭晓 王孙 -> 战\n",
      "宾 新 换镜 -> 谍报\n",
      "组织 慈善 赛格 -> 里\n",
      "梁痴人 说 梦末 -> 位龙\n",
      "凯特 温丝莱 特甩 -> 花心\n",
      "周欢 乐派 献饕 -> 餮盛\n",
      "历时 月 自费 -> 拍摄\n",
      "垂涎悍 复出 带来 -> 鲶鱼\n",
      "模式 机奥 南宁 -> 仅\n",
      "波拒 洋帅 老 -> 领导\n",
      "涨 停敢 死队 -> 火线\n",
      "逃亡 香格 里 -> 拉开\n",
      "快讯 阿里巴巴财年 净利 -> 亿\n",
      "乡村 基周三 午盘 -> 转涨\n",
      "令 连续 错过 -> 大牌\n",
      "城 综合 评价 -> 组图\n",
      "冯绍峰 杨幂 牵手 -> 荣膺\n",
      "复古 作 高像素 -> 富士\n",
      "要求 美军 停止 -> 空袭\n",
      "状告 法院 案孝感 -> 中院\n",
      "秘诀 裙子 一定 -> 迷你\n",
      "下挫 八大 机构 -> 后市\n",
      "世锦赛 征战 史梦 -> 队\n",
      "星巴克 供应 商塔塔 -> 咖啡拟\n",
      "基金 大佬 变动潮 -> 重现\n",
      "喝 活 周 -> 揭\n",
      "双模 轻松 上网 -> 天语\n",
      "委内瑞拉 有意 探索 -> 新\n",
      "东方 广场 五星级 -> 写字楼\n",
      "欧青赛 英格兰 逆转 -> 西班牙\n",
      "销量 数字 称 -> 遭\n",
      "接受 暧昧 尺度 -> 图\n",
      "绿色 源料 项目 -> 成功\n",
      "诈骗 实录 通讯 -> 网络\n",
      "陈冠希 冒死 亮相 -> 记者会\n",
      "转会 广厦 陈照升 -> 加盟\n",
      "北京 考门 实践 -> 课\n",
      "定位 商务 便携 -> 本售\n",
      "公证 妻子 引发 -> 多次\n",
      "香港 公开 赛次 -> 轮战\n",
      "高尔夫 巡回 赛月 -> 日\n",
      "激励 主题 投资 -> 迎来\n",
      "双色球 分布 图防 -> 升温\n",
      "科技 园国 科激光 -> 获\n",
      "风 吹动 小球 -> 球员\n",
      "银行 协助 制裁 -> 伊朗\n",
      "时 拒捕 刺死 -> 民警\n",
      "江山 孔杰 仅 -> 次\n",
      "逆转 功臣 质疑 -> 球迷\n",
      "亿 筹建 长江 -> 上游\n",
      "摄影师 拍到 小老鼠 -> 抢美洲\n",
      "感到 骄傲斯科拉 瞄准 -> 西南\n",
      "整固 右肩 行情 -> 成主\n",
      "品牌 大战 陷入 -> 定点\n",
      "花渐 欲迷人 眼 -> 游戏\n",
      "漂亮 妈妈 林熙蕾微 -> 博晒\n",
      "魔音 难 现荷 -> 甲\n",
      "警察 混入 偷袭 -> 至少\n",
      "炸弹 袭击 至少 -> 死亡\n",
      "企业 基因 突围 -> 关键\n",
      "整合 促进 业绩 -> 步入\n",
      "跳水 楼市 降价潮 -> 提前\n",
      "课 月 日起 -> 网报\n",
      "指数 初盘 埃弗顿 -> 需\n",
      "财季 利润 同比 -> 下滑\n",
      "大气 数码 摄像机 -> 仅\n",
      "议员 欲登 争议 -> 岛屿\n",
      "新娘 新婚 终日 -> 以泪洗面\n",
      "变革 暗流 志邦 -> 厨柜\n",
      "主席 暗示 裁判 -> 应\n",
      "总结 春节 过后 -> 公布\n",
      "晶欲 破 六千 -> 元\n",
      "后防 稳 排兵 -> 布阵\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型进行预测，train_input直接是上面代码中的，直接用\n",
    "# 模型输出之后取argmax，再用idx2token转回单词，查看效果，可以看到效果还可以，有上下文关系\n",
    "predict = model(train_input).data.max(1, keepdim=True)[1].squeeze_().tolist()\n",
    "input_list = train_input.tolist()\n",
    "for i in range(len(input_list)):\n",
    "    print(dataset.vocab.get_itos()[input_list[i][0]] + ' ' +  \n",
    "          dataset.vocab.get_itos()[input_list[i][1]] + ' ' + \n",
    "          dataset.vocab.get_itos()[input_list[i][2]] + ' -> ' + \n",
    "          dataset.vocab.get_itos()[predict[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d73fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
