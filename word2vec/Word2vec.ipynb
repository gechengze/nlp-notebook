{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad062b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb169b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, debug=False):\n",
    "        df = pd.read_csv('../../datasets/THUCNews/train.csv')\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        if debug:\n",
    "            df = df.sample(2000).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(50000).reset_index(drop=True)\n",
    "        # 读取常用停用词\n",
    "        stopwords = [line.strip() for line in open('../../stopwords/cn_stopwords.txt', 'r', encoding='utf-8').readlines()]\n",
    "        sentences = []\n",
    "        for title in df['title']:   \n",
    "            # 去除标点符号\n",
    "            title = re.sub(r'[^\\u4e00-\\u9fa5]', '', title)\n",
    "            # jieba分词\n",
    "            sentence_seged = jieba.cut(title.strip())    \n",
    "            outstr = ''\n",
    "            for word in sentence_seged:\n",
    "                if word != '\\t' and word not in stopwords:\n",
    "                    outstr += word\n",
    "                    outstr += ' '\n",
    "            if outstr != '':\n",
    "                sentences.append(outstr)    \n",
    "        # 获取所有词（token）\n",
    "        token_list = list(set(' '.join(sentences).split()))\n",
    "        # token和index互转字典\n",
    "        self.token2idx = {token: i for i, token in enumerate(token_list)}\n",
    "        self.idx2token = {i: token for i, token in enumerate(token_list)}\n",
    "        \n",
    "        self.vocab_size = len(self.token2idx)\n",
    "        # 构造输入和输出，跳元模型，用当前字预测前一个字和后一个字\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        for sen in sentences:\n",
    "            sen = sen.split()\n",
    "            for i in range(1, len(sen) - 1):\n",
    "                self.inputs.append([self.token2idx[sen[i]]])\n",
    "                self.labels.append([self.token2idx[sen[i - 1]]])\n",
    "                self.inputs.append([self.token2idx[sen[i]]])\n",
    "                self.labels.append([self.token2idx[sen[i + 1]]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.inputs[idx]), torch.LongTensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d651b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        # W和WT的形状是转置的\n",
    "        self.W = nn.Embedding(vocab_size, embed_size)  # vocab_size -> embed_size\n",
    "        self.WT = nn.Linear(embed_size, vocab_size, bias=False)  # embed_size -> vocab_size\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X形状：batch_size * vocab_size\n",
    "        hidden_layer = self.W(X)\n",
    "        output_layer = self.WT(hidden_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71631957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/d1/4_gsqv2176z583_7rmpm27lh0000gn/T/jieba.cache\n",
      "Loading model cost 0.414 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (W): Embedding(8299, 512)\n",
       "  (WT): Linear(in_features=512, out_features=8299, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "dataset = MyDataset(debug=True)\n",
    "# 构造dataloader，batch size设置为128\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "model = Word2Vec(vocab_size=len(dataset.token2idx), embed_size=512)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 查看模型\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edadaac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss = 9.057583\n",
      "epoch: 2 loss = 7.083758\n",
      "epoch: 3 loss = 5.226216\n",
      "epoch: 4 loss = 4.294971\n",
      "epoch: 5 loss = 3.998539\n",
      "epoch: 6 loss = 3.614333\n",
      "epoch: 7 loss = 3.471384\n",
      "epoch: 8 loss = 3.362638\n",
      "epoch: 9 loss = 3.157343\n",
      "epoch: 10 loss = 3.143133\n",
      "epoch: 11 loss = 3.154311\n",
      "epoch: 12 loss = 3.026036\n",
      "epoch: 13 loss = 2.999801\n",
      "epoch: 14 loss = 3.091376\n",
      "epoch: 15 loss = 2.960258\n",
      "epoch: 16 loss = 3.036891\n",
      "epoch: 17 loss = 3.065809\n",
      "epoch: 18 loss = 2.940340\n",
      "epoch: 19 loss = 2.946154\n",
      "epoch: 20 loss = 3.054584\n"
     ]
    }
   ],
   "source": [
    "# 训练20个epoch\n",
    "for epoch in range(20):\n",
    "    for train_input, train_label in dataloader:\n",
    "        output = model(train_input)\n",
    "        loss = criterion(output.squeeze_(), train_label.squeeze_())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch:', epoch + 1, 'loss =', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee9ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, WT = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55f9bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8299, 512]), torch.Size([8299, 512]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W对应vocab中每个词的vector，这里是512维\n",
    "W.shape, WT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630e497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
