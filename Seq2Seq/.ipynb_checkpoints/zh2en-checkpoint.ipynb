{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "my_vocab = {}\n",
    "counter = Counter()\n",
    "for string_ in df['zh']:\n",
    "    counter.update(list(string_))\n",
    "my_vocab['zh'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['zh'].set_default_index(my_vocab['zh']['<unk>'])\n",
    "\n",
    "counter = Counter()\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "for string_ in df['en']:\n",
    "    counter.update(en_tokenizer(string_))\n",
    "my_vocab['en'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['en'].set_default_index(my_vocab['en']['<unk>'])\n",
    "\n",
    "\n",
    "def data_process(df):\n",
    "    data = []\n",
    "    for raw_zh, raw_en in zip(df['zh'], df['en']):\n",
    "        zh_tensor_ = torch.tensor([my_vocab['zh'][token] for token in list(raw_zh)],\n",
    "                                  dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([my_vocab['en'][token] for token in en_tokenizer(raw_en)],\n",
    "                                  dtype=torch.long)\n",
    "        data.append((zh_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(df)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "PAD_IDX = my_vocab['zh']['<pad>']\n",
    "BOS_IDX = my_vocab['zh']['<bos>']\n",
    "EOS_IDX = my_vocab['zh']['<eos>']\n",
    "\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    zh_batch, en_batch = [], []\n",
    "    for zh_item, en_item in data_batch:\n",
    "        zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return zh_batch, en_batch\n",
    "\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec31f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        enc_output, enc_hidden = self.rnn(embedded)\n",
    "        return enc_output, enc_hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, hidden):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "        dec_output, hidden = self.rnn(embedded, hidden)\n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output, hidden = self.encoder(src)\n",
    "        max_len, batch_size = tgt.shape[0], tgt.shape[1]\n",
    "        output = torch.zeros(max_len, batch_size, self.decoder.vocab_size).to(device)\n",
    "        y = tgt[0, :]\n",
    "        for t in range(1, max_len):\n",
    "            y.unsqueeze_(0)\n",
    "            y, hidden = self.decoder(y, hidden)\n",
    "            y.squeeze_(0)\n",
    "            output[t] = y\n",
    "            y = y.max(1)[1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c198fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,162,824 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(vocab_size=len(my_vocab['zh']), embed_size=64, hidden_size=64)\n",
    "dec = Decoder(vocab_size=len(my_vocab['en']), embed_size=64, hidden_size=64)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "36e9cee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 2.142829372222165\n",
      "epoch: 2 , loss: 2.138854519430413\n",
      "epoch: 3 , loss: 2.1333219128918937\n",
      "epoch: 4 , loss: 2.129233419177044\n",
      "epoch: 5 , loss: 2.1305747218878874\n",
      "epoch: 6 , loss: 2.124239278126912\n",
      "epoch: 7 , loss: 2.116394963609167\n",
      "epoch: 8 , loss: 2.1189611748040442\n",
      "epoch: 9 , loss: 2.120151555681803\n",
      "epoch: 10 , loss: 2.1185890321271965\n",
      "epoch: 11 , loss: 2.1128992632210974\n",
      "epoch: 12 , loss: 2.1098639591630683\n",
      "epoch: 13 , loss: 2.1058094286056885\n",
      "epoch: 14 , loss: 2.106322065893426\n",
      "epoch: 15 , loss: 2.099632248821029\n",
      "epoch: 16 , loss: 2.0954696227269\n",
      "epoch: 17 , loss: 2.092597869505365\n",
      "epoch: 18 , loss: 2.0933723837496285\n",
      "epoch: 19 , loss: 2.090676277516836\n",
      "epoch: 20 , loss: 2.087188844221184\n",
      "epoch: 21 , loss: 2.0880146931452925\n",
      "epoch: 22 , loss: 2.0820474610271225\n",
      "epoch: 23 , loss: 2.0824243763843215\n",
      "epoch: 24 , loss: 2.0796353587185044\n",
      "epoch: 25 , loss: 2.0744919748191375\n",
      "epoch: 26 , loss: 2.0732915128570006\n",
      "epoch: 27 , loss: 2.069009046956717\n",
      "epoch: 28 , loss: 2.066514499216195\n",
      "epoch: 29 , loss: 2.064627055662224\n",
      "epoch: 30 , loss: 2.0610759229545135\n",
      "epoch: 31 , loss: 2.059531886893583\n",
      "epoch: 32 , loss: 2.0597962255937508\n",
      "epoch: 33 , loss: 2.0549696597708276\n",
      "epoch: 34 , loss: 2.055072416742164\n",
      "epoch: 35 , loss: 2.0546664430434447\n",
      "epoch: 36 , loss: 2.0480418162173537\n",
      "epoch: 37 , loss: 2.046202702694629\n",
      "epoch: 38 , loss: 2.0461679697036743\n",
      "epoch: 39 , loss: 2.038756738226098\n",
      "epoch: 40 , loss: 2.041611714535449\n",
      "epoch: 41 , loss: 2.040622388023928\n",
      "epoch: 42 , loss: 2.037106703562909\n",
      "epoch: 43 , loss: 2.032252038817808\n",
      "epoch: 44 , loss: 2.0325232658041528\n",
      "epoch: 45 , loss: 2.0272884627422654\n",
      "epoch: 46 , loss: 2.0247561357107506\n",
      "epoch: 47 , loss: 2.0203292728906654\n",
      "epoch: 48 , loss: 2.021399426173015\n",
      "epoch: 49 , loss: 2.0197573081556572\n",
      "epoch: 50 , loss: 2.0158927986420783\n",
      "epoch: 51 , loss: 2.0135268260197465\n",
      "epoch: 52 , loss: 2.009670003350959\n",
      "epoch: 53 , loss: 2.0074903979358902\n",
      "epoch: 54 , loss: 2.012304344809199\n",
      "epoch: 55 , loss: 2.0056106886231757\n",
      "epoch: 56 , loss: 2.006498603935701\n",
      "epoch: 57 , loss: 1.9969523125384228\n",
      "epoch: 58 , loss: 2.001123504466321\n",
      "epoch: 59 , loss: 1.9970094626208386\n",
      "epoch: 60 , loss: 1.9923595276223607\n",
      "epoch: 61 , loss: 1.9882842015071087\n",
      "epoch: 62 , loss: 1.9950914569647915\n",
      "epoch: 63 , loss: 1.9853711214410252\n",
      "epoch: 64 , loss: 1.99248344926949\n",
      "epoch: 65 , loss: 1.9830385762524891\n",
      "epoch: 66 , loss: 1.982000332280814\n",
      "epoch: 67 , loss: 1.9781890403793518\n",
      "epoch: 68 , loss: 1.9749340063118073\n",
      "epoch: 69 , loss: 1.975820662027382\n",
      "epoch: 70 , loss: 1.9738973838737213\n",
      "epoch: 71 , loss: 1.9689258696085\n",
      "epoch: 72 , loss: 1.971003825405994\n",
      "epoch: 73 , loss: 1.967979876391859\n",
      "epoch: 74 , loss: 1.9624337489346424\n",
      "epoch: 75 , loss: 1.9617672742131245\n",
      "epoch: 76 , loss: 1.9636551601340972\n",
      "epoch: 77 , loss: 1.9573363295520645\n",
      "epoch: 78 , loss: 1.9595800451485508\n",
      "epoch: 79 , loss: 1.9587925428367523\n",
      "epoch: 80 , loss: 1.9543128760464221\n",
      "epoch: 81 , loss: 1.9518461629568813\n",
      "epoch: 82 , loss: 1.9530943287424294\n",
      "epoch: 83 , loss: 1.9504287142351449\n",
      "epoch: 84 , loss: 1.9454803179545574\n",
      "epoch: 85 , loss: 1.9443185473062905\n",
      "epoch: 86 , loss: 1.9406945044735828\n",
      "epoch: 87 , loss: 1.9428523405488716\n",
      "epoch: 88 , loss: 1.9365771807819965\n",
      "epoch: 89 , loss: 1.9353915740208454\n",
      "epoch: 90 , loss: 1.931423415620643\n",
      "epoch: 91 , loss: 1.9296106013907008\n",
      "epoch: 92 , loss: 1.9362236031566757\n",
      "epoch: 93 , loss: 1.9305313121841614\n",
      "epoch: 94 , loss: 1.9261350631713867\n",
      "epoch: 95 , loss: 1.9200614446617035\n",
      "epoch: 96 , loss: 1.9225906076201473\n",
      "epoch: 97 , loss: 1.9204692424061787\n",
      "epoch: 98 , loss: 1.91711748795337\n",
      "epoch: 99 , loss: 1.9136827236198517\n",
      "epoch: 100 , loss: 1.9173473421349583\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_iter:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch:', epoch + 1, ', loss:', epoch_loss / len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c619df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def translate(zh, max_len=10):\n",
    "    zh_idx = [my_vocab['zh']['<bos>']] + my_vocab['zh'].lookup_indices(list(zh)) + [my_vocab['zh']['<eos>']]\n",
    "    zh_idx = torch.tensor(zh_idx, dtype=torch.long, device=device).unsqueeze_(1)\n",
    "    en_bos = my_vocab['en']['<bos>']\n",
    "    enc_output, hidden = model.encoder(zh_idx)\n",
    "    preds = []\n",
    "    y = torch.tensor([en_bos], dtype=torch.long, device=device)\n",
    "    for t in range(max_len):\n",
    "        y.unsqueeze_(1)\n",
    "        y, hidden = model.decoder(y, hidden)\n",
    "        y.squeeze_(1)\n",
    "        y = y.max(1)[1]\n",
    "        if y.item() == my_vocab['en']['<eos>']:\n",
    "            break\n",
    "        preds.append(my_vocab['en'].get_itos()[y.item()])\n",
    "    return ' '.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0a2d0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 'm a slight of student .\n"
     ]
    }
   ],
   "source": [
    "print(translate('我是一个学生'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dbd4efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗨。    ==>    Hi people\n",
      "你好。    ==>    You you . .\n",
      "你用跑的。    ==>    You made him .\n",
      "等等！    ==>    Wait waiting party waiting\n",
      "你好。    ==>    You you . .\n",
      "让我来。    ==>    Let me . . .\n",
      "我赢了。    ==>    I won won .\n",
      "不会吧。    ==>    Why are not .\n",
      "乾杯!    ==>    Cheers !\n",
      "你懂了吗？    ==>    Got you ? ?\n",
      "他跑了。    ==>    He turned it\n",
      "跳进来。    ==>    Hop in once\n",
      "我迷失了。    ==>    I lost lost person .\n",
      "我退出。    ==>    I confessed the car .\n",
      "我沒事。    ==>    I do n't serious . .\n",
      "听着。    ==>    Answer the party .\n",
      "不可能！    ==>    Stop ca understand !\n",
      "没门！    ==>    Shut open the\n",
      "你确定？    ==>    Really you . .\n",
      "试试吧。    ==>    Try it .\n",
      "我们来试试。    ==>    We 'll try another . .\n",
      "为什么是我？    ==>    Are you coming ?\n",
      "去问汤姆。    ==>    Tom Tom .\n",
      "冷静点。    ==>    Be late cold\n",
      "公平点。    ==>    Above of fair .\n",
      "友善点。    ==>    Dogs a sharp\n",
      "和气点。    ==>    Be a late\n",
      "联系我。    ==>    I me advice\n",
      "联系我们。    ==>    Answer us later\n",
      "进来。    ==>    Wait and straight back\n",
      "找到汤姆。    ==>    We Tom .\n",
      "滾出去！    ==>    Get out the . .\n",
      "出去！    ==>    Get to the .\n",
      "走開！    ==>    Go away !\n",
      "滾！    ==>    Is the hands ?\n",
      "走開！    ==>    Go away !\n",
      "再见！    ==>    Goodbye you again\n",
      "告辞！    ==>    Goodbye it\n",
      "等一下！    ==>    Wait it party\n",
      "他来了。    ==>    He came come\n",
      "他跑。    ==>    He dressed himself\n",
      "帮我一下。    ==>    Help me call .\n",
      "帮帮我们吧！    ==>    Help us .\n",
      "坚持。    ==>    Take it .\n",
      "抱抱汤姆！    ==>    Hug Tom .\n",
      "我同意。    ==>    I agree with the proposal\n",
      "我生病了。    ==>    I was sick sick .\n",
      "我老了。    ==>    I was a old .\n",
      "没关系。    ==>    Do n't you .\n",
      "是我。    ==>    Give me . .\n",
      "来加入我们吧。    ==>    Join us us\n",
      "留着吧。    ==>    Go it .\n",
      "吻我。    ==>    Kiss me .\n",
      "完美！    ==>    Perfect !\n",
      "再见！    ==>    Goodbye you again\n",
      "閉嘴！    ==>    Shut the business .\n",
      "不管它。    ==>    Skip it .\n",
      "拿走吧。    ==>    Take to to it .\n",
      "醒醒！    ==>    Wake it !\n",
      "去清洗一下。    ==>    Wash off the before .\n",
      "我们知道。    ==>    We know our truth .\n",
      "欢迎。    ==>    Welcome for\n",
      "谁赢了？    ==>    Who won ?\n",
      "为什么不？    ==>    Why n't . ?\n",
      "你跑。    ==>    You you right\n",
      "往后退点。    ==>    Drive at\n",
      "静静的，别动。    ==>    May I still .\n",
      "我一无所知。    ==>    I do n't know . .\n",
      "把他铐上。    ==>    Cuff him .\n",
      "往前开。    ==>    Drive on\n",
      "走開！    ==>    Go away !\n",
      "滾！    ==>    Is the hands ?\n",
      "趴下！    ==>    Get down the next .\n",
      "滾！    ==>    Is the hands ?\n",
      "醒醒吧。    ==>    Stay it .\n",
      "做得好！    ==>    Hi , !\n",
      "干的好！    ==>    Hi ! !\n",
      "抓住汤姆。    ==>    Grab Tom .\n",
      "抓住他。    ==>    Catch him .\n",
      "玩得開心。    ==>    Wait a a\n",
      "他来试试。    ==>    He tries .\n",
      "你就随了我的意吧。    ==>    Humor me me\n",
      "趕快!    ==>    Get up !\n",
      "快点！    ==>    Hurry up !\n",
      "我忘了。    ==>    I forgot to . .\n",
      "我放弃。    ==>    I resign . ?\n",
      "我來付錢。    ==>    I 'll pay the bill .\n",
      "我很忙。    ==>    I 'm busy busy busy\n",
      "我冷。    ==>    I have a cold .\n",
      "我很好。    ==>    I feel very very . .\n",
      "我吃飽了。    ==>    I 've been . .\n",
      "我生病了。    ==>    I was sick sick .\n",
      "我病了。    ==>    I was sick sick .\n",
      "我个子高。    ==>    I am tall tall . .\n",
      "让我一个人呆会儿。    ==>    I me the the tomorrow .\n",
      "走吧。    ==>    Let 's go . .\n",
      "我們開始吧！    ==>    Let me start !\n",
      "我們走吧!    ==>    Let 's !\n",
      "当心！    ==>    Good this !\n",
      "她跑。    ==>    She accepted it .\n"
     ]
    }
   ],
   "source": [
    "for zh in df['zh'][0: 100]:\n",
    "    print(zh, '   ==>   ', translate(zh, max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
