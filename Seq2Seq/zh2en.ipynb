{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cmn.txt', sep='\\t', header=None, names=['en', 'zh'])\n",
    "my_vocab = {}\n",
    "counter = Counter()\n",
    "for string_ in df['zh']:\n",
    "    counter.update(list(string_))\n",
    "my_vocab['zh'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['zh'].set_default_index(my_vocab['zh']['<unk>'])\n",
    "\n",
    "counter = Counter()\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "for string_ in df['en']:\n",
    "    counter.update(en_tokenizer(string_))\n",
    "my_vocab['en'] = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "my_vocab['en'].set_default_index(my_vocab['en']['<unk>'])\n",
    "\n",
    "\n",
    "def data_process(df):\n",
    "    data = []\n",
    "    for raw_zh, raw_en in zip(df['zh'], df['en']):\n",
    "        zh_tensor_ = torch.tensor([my_vocab['zh'][token] for token in list(raw_zh)],\n",
    "                                  dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([my_vocab['en'][token] for token in en_tokenizer(raw_en)],\n",
    "                                  dtype=torch.long)\n",
    "        data.append((zh_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(df)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "PAD_IDX = my_vocab['zh']['<pad>']\n",
    "BOS_IDX = my_vocab['zh']['<bos>']\n",
    "EOS_IDX = my_vocab['zh']['<eos>']\n",
    "\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    zh_batch, en_batch = [], []\n",
    "    for zh_item, en_item in data_batch:\n",
    "        zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return zh_batch, en_batch\n",
    "\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec31f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embed(x))\n",
    "        enc_output, enc_hidden = self.rnn(embedded)\n",
    "        return enc_output, enc_hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, hidden):\n",
    "        embedded = self.dropout(self.embed(y))\n",
    "        dec_output, hidden = self.rnn(embedded, hidden)\n",
    "        dec_output = self.fc(dec_output)\n",
    "        return dec_output, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output, hidden = self.encoder(src)\n",
    "        max_len, batch_size = tgt.shape[0], tgt.shape[1]\n",
    "        output = torch.zeros(max_len, batch_size, self.decoder.vocab_size)\n",
    "        y = tgt[0, :]\n",
    "        for t in range(1, max_len):\n",
    "            y.unsqueeze_(0)\n",
    "            y, hidden = self.decoder(y, hidden)\n",
    "            y.squeeze_(0)\n",
    "            output[t] = y\n",
    "            y = y.max(1)[1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c198fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,162,824 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(vocab_size=len(my_vocab['zh']), embed_size=64, hidden_size=64)\n",
    "dec = Decoder(vocab_size=len(my_vocab['en']), embed_size=64, hidden_size=64)\n",
    "\n",
    "model = Seq2Seq(enc, dec)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e9cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , loss: 5.165993232799299\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch:', epoch + 1, ', loss:', epoch_loss / len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410a4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
